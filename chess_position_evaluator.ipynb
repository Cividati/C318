{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-QvR6AmkEN2"
   },
   "source": [
    "# Avaliador de Posições de Xadrez\n",
    "\n",
    "O xadrez tem sido uma referência clássica para inteligência artificial, aprendizado de máquinas e mineração de dados desde sua criação. Devido às simples regras determinísticas do xadrez aliadas às possibilidades exponenciais e difíceis de avaliar posições, ganhou o foco de muitos pesquisadores em Ciência da Computação. Hoje em dia, existem vários IAs de xadrez sofisticados que competem em um nível acima até mesmo do mais forte Grande Mestre de xadrez do mundo. Enquanto as máquinas de xadrez como Stockfish (do qual este conjunto de dados é baseado) e AlphaGo são muito complexas para tentar competir aqui, vamos ver se podemos usar as avaliações de Stockfish para construir uma função de avaliação de posição comparável.\n",
    "\n",
    "Para este projeto estarei usando a biblioteca de pytorch e construindo com ela uma Rede Neural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEBD3tVxksVF",
    "outputId": "890346a9-8851-4e25-8d2b-473acfb808f4"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hyeeXkQXkEN4"
   },
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# for visualizing the results\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for reading input data\n",
    "import pandas as pd\n",
    "\n",
    "# for parsing the FEN of chess positions\n",
    "import re\n",
    "\n",
    "# Measuring time in seconds:\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import chess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAOmdyUIkEN5"
   },
   "source": [
    "Para representar uma posição de xadrez, é comum usar [Forsyth-Edwards Notation (FEN)](http://https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation) que contém todas as informações necessárias para reconstruir um jogo de xadrez a partir da posição atual. Para tornar estas informações utilizáveis para uma rede neural, usaremos um byte para representar se uma peça específica (torre branca, cavaleiro branco, etc...) está em um quadrado específico no tabuleiro de xadrez 8x8. Como há 6 peças diferentes e dois jogadores diferentes, isso significa que há 12 peças específicas que poderiam estar potencialmente em cada quadrado. \n",
    "\n",
    "No entanto, ainda precisamos manter um registro de informações como de quem é a vez, quais opções de castling ainda são legais, se en passant é possível, quantas meias jogadas desde uma jogada de peão ou captura de peças, e quantas jogadas o jogo já teve. Para fazer isso, usamos um tabuleiro adicional 8x8 onde as posições das torres representam direitos de castling, a 3ª e 6ª posições (fileira) mantêm registro de possíveis jogadas en passant, os e1 e e8 representam quem está em movimento, e a 4ª e 5ª posições representam o número de meias jogadas e jogadas completas como números binários (máximo possível é 255), respectivamente.\n",
    "\n",
    "Abaixo está uma função para fazer esta conversão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F-G2KNRQkEN6"
   },
   "outputs": [],
   "source": [
    "def fen_to_bit_vector(fen):\n",
    "    # piece placement - lowercase for black pieces, uppercase for white pieces. numbers represent consequtive spaces. / represents a new row \n",
    "    # active color - whose turn it is, either 'w' or 'b'\n",
    "    # castling rights - which castling moves are still legal K or k for kingside and Q or q for queenside, '-' if no legal castling moves for either player\n",
    "    # en passant - if the last move was a pawn moving up two squares, this is the space behind the square for the purposes of en passant\n",
    "    # halfmove clock - number of moves without a pawn move or piece capture, after 50 of which the game is a draw\n",
    "    # fullmove number - number of full turns starting at 1, increments after black's move\n",
    "\n",
    "    # Example FEN of starting position\n",
    "    # rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
    "    \n",
    "    parts = re.split(\" \", fen)\n",
    "    piece_placement = re.split(\"/\", parts[0])\n",
    "    active_color = parts[1]\n",
    "    castling_rights = parts[2]\n",
    "    en_passant = parts[3]\n",
    "    halfmove_clock = int(parts[4])\n",
    "    fullmove_clock = int(parts[5])\n",
    "\n",
    "    bit_vector = np.zeros((13, 8, 8), dtype=np.uint8)\n",
    "    \n",
    "    # piece to layer structure taken from reference [1]\n",
    "    piece_to_layer = {\n",
    "        'R': 1,\n",
    "        'N': 2,\n",
    "        'B': 3,\n",
    "        'Q': 4,\n",
    "        'K': 5,\n",
    "        'P': 6,\n",
    "        'p': 7,\n",
    "        'k': 8,\n",
    "        'q': 9,\n",
    "        'b': 10,\n",
    "        'n': 11,\n",
    "        'r': 12\n",
    "    }\n",
    "    \n",
    "    castling = {\n",
    "        'K': (7,7),\n",
    "        'Q': (7,0),\n",
    "        'k': (0,7),\n",
    "        'q': (0,0),\n",
    "    }\n",
    "\n",
    "    for r, row in enumerate(piece_placement):\n",
    "        c = 0\n",
    "        for piece in row:\n",
    "            if piece in piece_to_layer:\n",
    "                bit_vector[piece_to_layer[piece], r, c] = 1\n",
    "                c += 1\n",
    "            else:\n",
    "                c += int(piece)\n",
    "    \n",
    "    if en_passant != '-':\n",
    "        bit_vector[0, ord(en_passant[0]) - ord('a'), int(en_passant[1]) - 1] = 1\n",
    "    \n",
    "    if castling_rights != '-':\n",
    "        for char in castling_rights:\n",
    "            bit_vector[0, castling[char][0], castling[char][1]] = 1\n",
    "    \n",
    "    if active_color == 'w':\n",
    "        bit_vector[0, 7, 4] = 1\n",
    "    else:\n",
    "        bit_vector[0, 0, 4] = 1\n",
    "\n",
    "    if halfmove_clock > 0:\n",
    "        c = 7\n",
    "        while halfmove_clock > 0:\n",
    "            bit_vector[0, 3, c] = halfmove_clock%2\n",
    "            halfmove_clock = halfmove_clock // 2\n",
    "            c -= 1\n",
    "            if c < 0:\n",
    "                break\n",
    "\n",
    "    if fullmove_clock > 0:\n",
    "        c = 7\n",
    "        while fullmove_clock > 0:\n",
    "            bit_vector[0, 4, c] = fullmove_clock%2\n",
    "            fullmove_clock = fullmove_clock // 2\n",
    "            c -= 1\n",
    "            if c < 0:\n",
    "                break\n",
    "\n",
    "    return bit_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76SVquvLkEN7",
    "outputId": "f4f58f4b-7405-4bf6-ddbc-92a68fc81627"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>r n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\nP P P P P P P P\nR N B Q K B N R</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(60, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(285, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(60, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(105, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(195, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(240, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(285, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(330, 15)\" /></svg>",
      "text/plain": [
       "Board('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
    "\n",
    "chess.Board(fen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JCSr6-wkEN8"
   },
   "source": [
    "O primeiro tabuleiro 8x8 (0º índice) contém todas as informações extras e os 12 tabuleiros seguintes (1 a 12) representam a localização das peças na ordem \n",
    "\n",
    "1. Torre branca\n",
    "2. Cavaleiro Branco\n",
    "3. Bispo Branco\n",
    "4. Rainha Branca\n",
    "5. Rei Branco\n",
    "6. Peão Branco\n",
    "7. Peão Preto\n",
    "8. Rei Negro\n",
    "9. Rainha Negra\n",
    "10. Bispo negro\n",
    "11. Cavaleiro Negro\n",
    "12. Torre Negra\n",
    "\n",
    "Observe como as peças se alinham corretamente com a posição inicial com o primeiro tabuleiro indicando corretamente que está na vez da branca se mover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vf5IjQEKkEN8"
   },
   "source": [
    "# Rede Neural\n",
    "\n",
    "Começaremos com uma simples Rede Neural Feed-Forward que está totalmente conectada. As Redes Neurais são nomeadas por sua estrutura ser análoga à dos neurônios no cérebro humano. A idéia é que, no cérebro humano, quando um neurônio recebe um impulso elétrico através de suas sinapses, ele às vezes dispara um impulso elétrico para outros neurônios, criando uma reação em cadeia. Para as redes neurais, nossos neurônios são nós, nossas sinapses são bordas (com pesos correspondentes) e a queima do neurônio é a função de ativação e saída do nó. \n",
    "\n",
    "![Perceptron](http://starship-knowledge.com/wp-content/uploads/2020/10/Perceptrons-1024x724.jpeg)\n",
    "\n",
    "O objetivo da Rede Neural é ter pesos tais que, após todas as reações em cadeia dos nós que recebem entradas e produzem saídas, a saída de informações do nó final represente a avaliação da posição do xadrez que iniciou o processo. Para encontrar realmente tais pesos, usaremos um método conhecido como retropropagação, que ajusta iterativamente os pesos na rede para aproximar a saída da resposta que desejamos.\n",
    "\n",
    "Tecnicamente falando, para cada registro de treinamento (um FEN e uma avaliação) inserimos a posição na Rede Neural, e após obter um resultado, calculamos o erro entre o resultado e a avaliação correta que pode ser representada como uma função de erro. Para alterar os pesos de forma a minimizar esta função de erro, calculamos o gradiente da função de erro e ajustamos os pesos na direção oposta. Isto significa que, se excedermos, queremos diminuir nossa avaliação e, se não conseguirmos, queremos aumentar nossa avaliação.\n",
    "\n",
    "![Gradiente de descida](https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_gradient_descent_1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4UUqb2VIkEN9"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(832, 832)\n",
    "        self.fc2 = nn.Linear(832, 416)\n",
    "        self.fc3 = nn.Linear(416, 208)\n",
    "        self.fc4 = nn.Linear(208, 104)\n",
    "        self.fc5 = nn.Linear(104, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "brRrxjBBkEN9"
   },
   "outputs": [],
   "source": [
    "# ChessDataset code and eval_to_int code taken from reference [1]\n",
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, data_frame):\n",
    "        self.fens = torch.from_numpy(np.array([*map(fen_to_bit_vector, data_frame[\"FEN\"])], dtype=np.float32))\n",
    "        self.evals = torch.Tensor([[x] for x in data_frame[\"Evaluation\"]])\n",
    "        self._len = len(self.evals)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.fens[index], self.evals[index]\n",
    "\n",
    "\n",
    "def eval_to_int(evaluation):\n",
    "    try:\n",
    "        res = int(evaluation)\n",
    "    except ValueError:\n",
    "        res = 10000 if evaluation[1] == '+' else -10000\n",
    "    return res / 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jU5Ab8TGkEN-"
   },
   "outputs": [],
   "source": [
    "def AdamW_main(trainloader, testloader, batch_size, epochs, device):\n",
    "    \n",
    "    net = Net().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(net.parameters())\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            # if i == len(trainloader):    # print every 2000 mini-batches\n",
    "            #     # denominator for loss should represent the number of positions evaluated \n",
    "            #     # independent of the batch size\n",
    "            #     print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (1000*len(labels))))\n",
    "            #     running_loss = 0.0\n",
    "\n",
    "            # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (1000*len(labels))))\n",
    "        # print('epoch: %d - running loss: %.5f'% (epoch, running_loss/len(trainset)))\n",
    "        running_loss = 0.0\n",
    "            \n",
    "    # print('Finished Training')\n",
    "\n",
    "    # PATH = './chess.pth'\n",
    "    # torch.save(net.state_dict(), PATH)\n",
    "\n",
    "    # print('Evaluating model')\n",
    "\n",
    "    count = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n",
    "            \n",
    "            # count should represent the number of positions evaluated \n",
    "            # independent of the batch size\n",
    "            count += len(labels)\n",
    "         \n",
    "            # if count % 10000 == 0:\n",
    "            #     print('model on the {} tactics positions is {}'.format(count, loss/count))\n",
    "    print('AdamW MSE: %.5f'%(loss/count))\n",
    "    return 'AdamW MSE: %.5f'%(loss/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JdXc0GqCkEN_"
   },
   "outputs": [],
   "source": [
    "def SGD_main(trainloader, testloader, batch_size, epochs, device, learning_rate):\n",
    "\n",
    "    net = Net().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr = learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            # if i % 2000 == 0:    # print every 1000 mini-batches\n",
    "            #     # denominator for loss should represent the number of positions evaluated \n",
    "            #     # independent of the batch size\n",
    "            #     print('[%d, %d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (1000*len(labels))))\n",
    "            #     running_loss = 0.0\n",
    "#         print('epoch {} - running_loss {} - len(trainset) {}'.format(epoch, running_loss, len(trainset)))\n",
    "        # print('epoch: %d - running loss: %.5f' % (epoch, float(running_loss/len(trainset))))\n",
    "#         print('epoch: {} - running loss: {}'.format(epoch, running_loss/dataSize))\n",
    "        running_loss = 0.0\n",
    "\n",
    "    # print('Finished Training')\n",
    "\n",
    "    # PATH = './chess.pth'\n",
    "    # torch.save(net.state_dict(), PATH)\n",
    "\n",
    "    # print('Evaluating model')\n",
    "\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n",
    "            \n",
    "            # count should represent the number of positions evaluated \n",
    "            # independent of the batch size\n",
    "            count += len(labels)\n",
    "            total_loss += loss\n",
    "            # if count % 10000 == 0:\n",
    "            #     print('model on the {} tactics positions is {}'.format(count, loss/count))\n",
    "    print('SGD %.3f MSE: %.5f'%(learning_rate, loss/count))\n",
    "    return 'SGD %.3f MSE: %.5f'%(learning_rate, loss/count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Training Data...\n",
      "Preparing Test Data...\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = './'\n",
    "print(\"Preparing Training Data...\")\n",
    "train_data = pd.read_csv(FILE_PATH + \"chessData.csv\")\n",
    "# print(\"total chessData size:\",train_data.shape[0]/10**6,\"m (\", train_data.shape[0],\")\")\n",
    "\n",
    "print(\"Preparing Test Data...\")\n",
    "test_data = pd.read_csv(FILE_PATH + \"tactic_evals.csv\")\n",
    "# print(\"total: tactic_evals size:\",test_data.shape[0]//10**6,\"m(\", test_data.shape[0],\")\")\n",
    "\n",
    "def run_models(MAX_DATA, BATCH_SIZE = 10, EPOCHS = 10, DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), train_data=train_data, test_data=test_data):\n",
    "    \n",
    "    # print(\"Using device {}\".format(DEVICE))\n",
    "\n",
    "    print(\"Preparing Training Data...\")\n",
    "    train_data = train_data[:MAX_DATA]\n",
    "    train_data[\"Evaluation\"] = train_data[\"Evaluation\"].map(eval_to_int)\n",
    "    trainset = ChessDataset(train_data)\n",
    "\n",
    "    print(\"Preparing Test Data...\")\n",
    "    test_data = test_data[:MAX_DATA]\n",
    "    test_data[\"Evaluation\"] = test_data[\"Evaluation\"].map(eval_to_int)\n",
    "    testset = ChessDataset(test_data)\n",
    "\n",
    "    # print(\"Converting to pytorch Dataset...\")\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)    \n",
    "    \n",
    "    print(\"total data: {}\".format(len(trainset)))\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    print(\"AdamW\")\n",
    "    start = timer()\n",
    "    AdamW = AdamW_main(trainloader, testloader, BATCH_SIZE, EPOCHS, DEVICE)\n",
    "    end = timer()\n",
    "    print('time elapsed on AdamW:', timedelta(seconds=end-start))\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    print(\"SGD 0.01\")\n",
    "    start = timer()\n",
    "    SGD_01 = SGD_main(trainloader, testloader, BATCH_SIZE, EPOCHS, DEVICE, 0.01)\n",
    "    end = timer()\n",
    "    print('time elapsed on SGD 0.01:', timedelta(seconds=end-start))\n",
    "\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    print(\"SGD 0.001\")\n",
    "    start = timer()\n",
    "    SGD_001 = SGD_main(trainloader, testloader, BATCH_SIZE, EPOCHS, DEVICE, 0.001)\n",
    "    end = timer()\n",
    "    print('time elapsed on SGD - 0.001:', timedelta(seconds=end-start))\n",
    "\n",
    "    print(\"-\"*50)\n",
    "    params = f\"MAX DATA: {MAX_DATA} - BATCH_SIZE: {BATCH_SIZE} EPOCHS: {EPOCHS}\"\n",
    "    print(params)\n",
    "    print(AdamW) \n",
    "    print(SGD_01) \n",
    "    print(SGD_001)\n",
    "    return (params, AdamW, SGD_01, SGD_001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Training Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rubens\\AppData\\Local\\Temp\\ipykernel_16724\\2460897923.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[\"Evaluation\"] = train_data[\"Evaluation\"].map(eval_to_int)\n",
      "C:\\Users\\Rubens\\AppData\\Local\\Temp\\ipykernel_16724\\2460897923.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[\"Evaluation\"] = test_data[\"Evaluation\"].map(eval_to_int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 698.16925\n",
      "time elapsed on AdamW: 0:00:01.630504\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 700.33087\n",
      "time elapsed on SGD 0.01: 0:00:00.023235\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 701.72784\n",
      "time elapsed on SGD - 0.001: 0:00:00.022358\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 10 EPOCHS: 10\n",
      "AdamW MSE: 698.16925\n",
      "SGD 0.010 MSE: 700.33087\n",
      "SGD 0.001 MSE: 701.72784\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 695.39447\n",
      "time elapsed on AdamW: 0:00:00.057780\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 698.33923\n",
      "time elapsed on SGD 0.01: 0:00:00.040599\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 700.76721\n",
      "time elapsed on SGD - 0.001: 0:00:00.041716\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 10 EPOCHS: 20\n",
      "AdamW MSE: 695.39447\n",
      "SGD 0.010 MSE: 698.33923\n",
      "SGD 0.001 MSE: 700.76721\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 695.25702\n",
      "time elapsed on AdamW: 0:00:00.082011\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 698.44141\n",
      "time elapsed on SGD 0.01: 0:00:00.059903\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 701.47235\n",
      "time elapsed on SGD - 0.001: 0:00:00.057924\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 10 EPOCHS: 30\n",
      "AdamW MSE: 695.25702\n",
      "SGD 0.010 MSE: 698.44141\n",
      "SGD 0.001 MSE: 701.47235\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 698.07172\n",
      "time elapsed on AdamW: 0:00:00.031637\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 699.46838\n",
      "time elapsed on SGD 0.01: 0:00:00.023823\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 701.11859\n",
      "time elapsed on SGD - 0.001: 0:00:00.024721\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 20 EPOCHS: 10\n",
      "AdamW MSE: 698.07172\n",
      "SGD 0.010 MSE: 699.46838\n",
      "SGD 0.001 MSE: 701.11859\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 697.20831\n",
      "time elapsed on AdamW: 0:00:00.052830\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 698.54059\n",
      "time elapsed on SGD 0.01: 0:00:00.042283\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 700.93719\n",
      "time elapsed on SGD - 0.001: 0:00:00.040933\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 20 EPOCHS: 20\n",
      "AdamW MSE: 697.20831\n",
      "SGD 0.010 MSE: 698.54059\n",
      "SGD 0.001 MSE: 700.93719\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 695.44049\n",
      "time elapsed on AdamW: 0:00:00.076851\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 698.23962\n",
      "time elapsed on SGD 0.01: 0:00:00.053376\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 699.45856\n",
      "time elapsed on SGD - 0.001: 0:00:00.057302\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 20 EPOCHS: 30\n",
      "AdamW MSE: 695.44049\n",
      "SGD 0.010 MSE: 698.23962\n",
      "SGD 0.001 MSE: 699.45856\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 698.05487\n",
      "time elapsed on AdamW: 0:00:00.031720\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 700.14417\n",
      "time elapsed on SGD 0.01: 0:00:00.027515\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 701.54150\n",
      "time elapsed on SGD - 0.001: 0:00:00.022407\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 30 EPOCHS: 10\n",
      "AdamW MSE: 698.05487\n",
      "SGD 0.010 MSE: 700.14417\n",
      "SGD 0.001 MSE: 701.54150\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 697.68970\n",
      "time elapsed on AdamW: 0:00:00.053915\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 698.79584\n",
      "time elapsed on SGD 0.01: 0:00:00.044611\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 699.41534\n",
      "time elapsed on SGD - 0.001: 0:00:00.039689\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 30 EPOCHS: 20\n",
      "AdamW MSE: 697.68970\n",
      "SGD 0.010 MSE: 698.79584\n",
      "SGD 0.001 MSE: 699.41534\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 10\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 696.19623\n",
      "time elapsed on AdamW: 0:00:00.079526\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 697.71326\n",
      "time elapsed on SGD 0.01: 0:00:00.055894\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 701.54608\n",
      "time elapsed on SGD - 0.001: 0:00:00.055401\n",
      "--------------------------------------------------\n",
      "MAX DATA: 10 - BATCH_SIZE: 30 EPOCHS: 30\n",
      "AdamW MSE: 696.19623\n",
      "SGD 0.010 MSE: 697.71326\n",
      "SGD 0.001 MSE: 701.54608\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 349.16797\n",
      "time elapsed on AdamW: 0:00:00.053403\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 349.48260\n",
      "time elapsed on SGD 0.01: 0:00:00.037600\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.21664\n",
      "time elapsed on SGD - 0.001: 0:00:00.040180\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 10 EPOCHS: 10\n",
      "AdamW MSE: 349.16797\n",
      "SGD 0.010 MSE: 349.48260\n",
      "SGD 0.001 MSE: 350.21664\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 348.00534\n",
      "time elapsed on AdamW: 0:00:00.128448\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 349.03430\n",
      "time elapsed on SGD 0.01: 0:00:00.092132\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.08548\n",
      "time elapsed on SGD - 0.001: 0:00:00.085380\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 10 EPOCHS: 20\n",
      "AdamW MSE: 348.00534\n",
      "SGD 0.010 MSE: 349.03430\n",
      "SGD 0.001 MSE: 350.08548\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 348.27158\n",
      "time elapsed on AdamW: 0:00:00.185404\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 348.56894\n",
      "time elapsed on SGD 0.01: 0:00:00.104691\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.24173\n",
      "time elapsed on SGD - 0.001: 0:00:00.102066\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 10 EPOCHS: 30\n",
      "AdamW MSE: 348.27158\n",
      "SGD 0.010 MSE: 348.56894\n",
      "SGD 0.001 MSE: 350.24173\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 348.56876\n",
      "time elapsed on AdamW: 0:00:00.031925\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 349.48209\n",
      "time elapsed on SGD 0.01: 0:00:00.027603\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.55252\n",
      "time elapsed on SGD - 0.001: 0:00:00.026477\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 20 EPOCHS: 10\n",
      "AdamW MSE: 348.56876\n",
      "SGD 0.010 MSE: 349.48209\n",
      "SGD 0.001 MSE: 350.55252\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 348.63776\n",
      "time elapsed on AdamW: 0:00:00.054428\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 349.06665\n",
      "time elapsed on SGD 0.01: 0:00:00.041889\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.31665\n",
      "time elapsed on SGD - 0.001: 0:00:00.044209\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 20 EPOCHS: 20\n",
      "AdamW MSE: 348.63776\n",
      "SGD 0.010 MSE: 349.06665\n",
      "SGD 0.001 MSE: 350.31665\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 347.29517\n",
      "time elapsed on AdamW: 0:00:00.084087\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 348.58118\n",
      "time elapsed on SGD 0.01: 0:00:00.055405\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.57693\n",
      "time elapsed on SGD - 0.001: 0:00:00.057465\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 20 EPOCHS: 30\n",
      "AdamW MSE: 347.29517\n",
      "SGD 0.010 MSE: 348.58118\n",
      "SGD 0.001 MSE: 350.57693\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 348.93048\n",
      "time elapsed on AdamW: 0:00:00.032613\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 349.90695\n",
      "time elapsed on SGD 0.01: 0:00:00.024728\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.66010\n",
      "time elapsed on SGD - 0.001: 0:00:00.025195\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 30 EPOCHS: 10\n",
      "AdamW MSE: 348.93048\n",
      "SGD 0.010 MSE: 349.90695\n",
      "SGD 0.001 MSE: 350.66010\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 348.11102\n",
      "time elapsed on AdamW: 0:00:00.055743\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 348.78940\n",
      "time elapsed on SGD 0.01: 0:00:00.039667\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 350.45670\n",
      "time elapsed on SGD - 0.001: 0:00:00.041931\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 30 EPOCHS: 20\n",
      "AdamW MSE: 348.11102\n",
      "SGD 0.010 MSE: 348.78940\n",
      "SGD 0.001 MSE: 350.45670\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 20\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 347.98196\n",
      "time elapsed on AdamW: 0:00:00.077934\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 348.56235\n",
      "time elapsed on SGD 0.01: 0:00:00.059071\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 349.76938\n",
      "time elapsed on SGD - 0.001: 0:00:00.065523\n",
      "--------------------------------------------------\n",
      "MAX DATA: 20 - BATCH_SIZE: 30 EPOCHS: 30\n",
      "AdamW MSE: 347.98196\n",
      "SGD 0.010 MSE: 348.56235\n",
      "SGD 0.001 MSE: 349.76938\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 134.83347\n",
      "time elapsed on AdamW: 0:00:00.079583\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 135.21613\n",
      "time elapsed on SGD 0.01: 0:00:00.062799\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 134.00056\n",
      "time elapsed on SGD - 0.001: 0:00:00.060298\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 10 EPOCHS: 10\n",
      "AdamW MSE: 134.83347\n",
      "SGD 0.010 MSE: 135.21613\n",
      "SGD 0.001 MSE: 134.00056\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 134.64110\n",
      "time elapsed on AdamW: 0:00:00.152071\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 135.52898\n",
      "time elapsed on SGD 0.01: 0:00:00.103947\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 133.96735\n",
      "time elapsed on SGD - 0.001: 0:00:00.101297\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 10 EPOCHS: 20\n",
      "AdamW MSE: 134.64110\n",
      "SGD 0.010 MSE: 135.52898\n",
      "SGD 0.001 MSE: 133.96735\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 134.52380\n",
      "time elapsed on AdamW: 0:00:00.213872\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 135.80301\n",
      "time elapsed on SGD 0.01: 0:00:00.148853\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 134.28987\n",
      "time elapsed on SGD - 0.001: 0:00:00.149110\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 10 EPOCHS: 30\n",
      "AdamW MSE: 134.52380\n",
      "SGD 0.010 MSE: 135.80301\n",
      "SGD 0.001 MSE: 134.28987\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 134.96245\n",
      "time elapsed on AdamW: 0:00:00.140607\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 134.77951\n",
      "time elapsed on SGD 0.01: 0:00:00.040981\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 133.84547\n",
      "time elapsed on SGD - 0.001: 0:00:00.041705\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 20 EPOCHS: 10\n",
      "AdamW MSE: 134.96245\n",
      "SGD 0.010 MSE: 134.77951\n",
      "SGD 0.001 MSE: 133.84547\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 134.64735\n",
      "time elapsed on AdamW: 0:00:00.099217\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 135.21452\n",
      "time elapsed on SGD 0.01: 0:00:00.068781\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 134.24974\n",
      "time elapsed on SGD - 0.001: 0:00:00.074302\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 20 EPOCHS: 20\n",
      "AdamW MSE: 134.64735\n",
      "SGD 0.010 MSE: 135.21452\n",
      "SGD 0.001 MSE: 134.24974\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 134.61244\n",
      "time elapsed on AdamW: 0:00:00.144523\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 135.62642\n",
      "time elapsed on SGD 0.01: 0:00:00.115596\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 133.92093\n",
      "time elapsed on SGD - 0.001: 0:00:00.112932\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 20 EPOCHS: 30\n",
      "AdamW MSE: 134.61244\n",
      "SGD 0.010 MSE: 135.62642\n",
      "SGD 0.001 MSE: 133.92093\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 199.70473\n",
      "time elapsed on AdamW: 0:00:00.033540\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 199.98520\n",
      "time elapsed on SGD 0.01: 0:00:00.025728\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 200.19804\n",
      "time elapsed on SGD - 0.001: 0:00:00.026014\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 30 EPOCHS: 10\n",
      "AdamW MSE: 199.70473\n",
      "SGD 0.010 MSE: 199.98520\n",
      "SGD 0.001 MSE: 200.19804\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 199.54568\n",
      "time elapsed on AdamW: 0:00:00.059728\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 199.59634\n",
      "time elapsed on SGD 0.01: 0:00:00.045080\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 200.31293\n",
      "time elapsed on SGD - 0.001: 0:00:00.042370\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 30 EPOCHS: 20\n",
      "AdamW MSE: 199.54568\n",
      "SGD 0.010 MSE: 199.59634\n",
      "SGD 0.001 MSE: 200.31293\n",
      "--------------------------------------------------\n",
      "Preparing Training Data...\n",
      "Preparing Test Data...\n",
      "total data: 30\n",
      "--------------------------------------------------\n",
      "AdamW\n",
      "AdamW MSE: 199.24232\n",
      "time elapsed on AdamW: 0:00:00.084398\n",
      "--------------------------------------------------\n",
      "SGD 0.01\n",
      "SGD 0.010 MSE: 199.55093\n",
      "time elapsed on SGD 0.01: 0:00:00.061787\n",
      "--------------------------------------------------\n",
      "SGD 0.001\n",
      "SGD 0.001 MSE: 200.09076\n",
      "time elapsed on SGD - 0.001: 0:00:00.058366\n",
      "--------------------------------------------------\n",
      "MAX DATA: 30 - BATCH_SIZE: 30 EPOCHS: 30\n",
      "AdamW MSE: 199.24232\n",
      "SGD 0.010 MSE: 199.55093\n",
      "SGD 0.001 MSE: 200.09076\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "_MAX_DATA = [10, 20, 30]\n",
    "_BATCH_SIZE = [10, 20, 30]\n",
    "_EPOCHS = [10, 20, 30]\n",
    "results = []\n",
    "for m in range(0,3):\n",
    "    # Permutatig MAX DATA\n",
    "    for b in range(0,3):\n",
    "        # Permutating BATCH_SIZE\n",
    "        for e in range(0,3):\n",
    "            # Permutating EPOCHS\n",
    "            # print(\"MAX_DATA:\",_MAX_DATA[m],\"BATCH_SIZE:\",_BATCH_SIZE[b],\"EPOCHS:\",_EPOCHS[e])\n",
    "            result = run_models(_MAX_DATA[m], BATCH_SIZE=_BATCH_SIZE[b], EPOCHS=_EPOCHS[e])\n",
    "            results.append(result)\n",
    "            print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MAX DATA: 10 - BATCH_SIZE: 10 EPOCHS: 10', 'AdamW MSE: 698.16925', 'SGD 0.010 MSE: 700.33087', 'SGD 0.001 MSE: 701.72784')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 10 EPOCHS: 20', 'AdamW MSE: 695.39447', 'SGD 0.010 MSE: 698.33923', 'SGD 0.001 MSE: 700.76721')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 10 EPOCHS: 30', 'AdamW MSE: 695.25702', 'SGD 0.010 MSE: 698.44141', 'SGD 0.001 MSE: 701.47235')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 20 EPOCHS: 10', 'AdamW MSE: 698.07172', 'SGD 0.010 MSE: 699.46838', 'SGD 0.001 MSE: 701.11859')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 20 EPOCHS: 20', 'AdamW MSE: 697.20831', 'SGD 0.010 MSE: 698.54059', 'SGD 0.001 MSE: 700.93719')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 20 EPOCHS: 30', 'AdamW MSE: 695.44049', 'SGD 0.010 MSE: 698.23962', 'SGD 0.001 MSE: 699.45856')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 30 EPOCHS: 10', 'AdamW MSE: 698.05487', 'SGD 0.010 MSE: 700.14417', 'SGD 0.001 MSE: 701.54150')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 30 EPOCHS: 20', 'AdamW MSE: 697.68970', 'SGD 0.010 MSE: 698.79584', 'SGD 0.001 MSE: 699.41534')\n",
      "('MAX DATA: 10 - BATCH_SIZE: 30 EPOCHS: 30', 'AdamW MSE: 696.19623', 'SGD 0.010 MSE: 697.71326', 'SGD 0.001 MSE: 701.54608')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 10 EPOCHS: 10', 'AdamW MSE: 349.16797', 'SGD 0.010 MSE: 349.48260', 'SGD 0.001 MSE: 350.21664')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 10 EPOCHS: 20', 'AdamW MSE: 348.00534', 'SGD 0.010 MSE: 349.03430', 'SGD 0.001 MSE: 350.08548')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 10 EPOCHS: 30', 'AdamW MSE: 348.27158', 'SGD 0.010 MSE: 348.56894', 'SGD 0.001 MSE: 350.24173')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 20 EPOCHS: 10', 'AdamW MSE: 348.56876', 'SGD 0.010 MSE: 349.48209', 'SGD 0.001 MSE: 350.55252')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 20 EPOCHS: 20', 'AdamW MSE: 348.63776', 'SGD 0.010 MSE: 349.06665', 'SGD 0.001 MSE: 350.31665')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 20 EPOCHS: 30', 'AdamW MSE: 347.29517', 'SGD 0.010 MSE: 348.58118', 'SGD 0.001 MSE: 350.57693')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 30 EPOCHS: 10', 'AdamW MSE: 348.93048', 'SGD 0.010 MSE: 349.90695', 'SGD 0.001 MSE: 350.66010')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 30 EPOCHS: 20', 'AdamW MSE: 348.11102', 'SGD 0.010 MSE: 348.78940', 'SGD 0.001 MSE: 350.45670')\n",
      "('MAX DATA: 20 - BATCH_SIZE: 30 EPOCHS: 30', 'AdamW MSE: 347.98196', 'SGD 0.010 MSE: 348.56235', 'SGD 0.001 MSE: 349.76938')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 10 EPOCHS: 10', 'AdamW MSE: 134.83347', 'SGD 0.010 MSE: 135.21613', 'SGD 0.001 MSE: 134.00056')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 10 EPOCHS: 20', 'AdamW MSE: 134.64110', 'SGD 0.010 MSE: 135.52898', 'SGD 0.001 MSE: 133.96735')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 10 EPOCHS: 30', 'AdamW MSE: 134.52380', 'SGD 0.010 MSE: 135.80301', 'SGD 0.001 MSE: 134.28987')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 20 EPOCHS: 10', 'AdamW MSE: 134.96245', 'SGD 0.010 MSE: 134.77951', 'SGD 0.001 MSE: 133.84547')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 20 EPOCHS: 20', 'AdamW MSE: 134.64735', 'SGD 0.010 MSE: 135.21452', 'SGD 0.001 MSE: 134.24974')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 20 EPOCHS: 30', 'AdamW MSE: 134.61244', 'SGD 0.010 MSE: 135.62642', 'SGD 0.001 MSE: 133.92093')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 30 EPOCHS: 10', 'AdamW MSE: 199.70473', 'SGD 0.010 MSE: 199.98520', 'SGD 0.001 MSE: 200.19804')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 30 EPOCHS: 20', 'AdamW MSE: 199.54568', 'SGD 0.010 MSE: 199.59634', 'SGD 0.001 MSE: 200.31293')\n",
      "('MAX DATA: 30 - BATCH_SIZE: 30 EPOCHS: 30', 'AdamW MSE: 199.24232', 'SGD 0.010 MSE: 199.55093', 'SGD 0.001 MSE: 200.09076')\n"
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ft_BO1EkEN-"
   },
   "source": [
    "# Resultados de AdamW\n",
    "\n",
    "Um erro médio de 0,0225 parece muito razoável, porém isto não é tão surpreendente quanto poderia parecer a princípio, já que as posições onde a avaliação do stockfish não são um par forçado, foram normalizadas para uma faixa de aproximadamente -1,5 a 1,5 e as posições onde a avaliação do stockfish é um par forçado foram definidas para -100 ou 100. Por convenção, uma pontuação positiva indica uma vantagem para o branco e uma pontuação negativa indica uma vantagem para o preto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqaMOztFkEN_"
   },
   "source": [
    "# Testando com SGD\n",
    "\n",
    "Originalmente usamos o algoritmo AdamW para a etapa de otimização, mas e se tentarmos a Descendência Estocástica Gradiente (SGD)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToqT2gEAkEOA"
   },
   "source": [
    "Um erro médio de 0,0207 é uma ligeira melhoria, mas não tanto que possamos dizer definitivamente que o SGD fez melhor do que AdamW. Como a perda de treinamento não diminuiu muito ao longo das 10 épocas, é muito mais provável que o SGD seja um otimizador pior para este problema de classificação ou que a taxa de aprendizado seja muito grande, o que está impedindo o modelo de convergir para o ótimo local. Vamos tentar novamente, mas com uma taxa de aprendizado de 0,001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhpZCgdzkEOA"
   },
   "source": [
    "O erro de treinamento parece estar diminuindo mesmo que esporádico, mas o erro de classificação ficou um pouco pior. Parece que o AdamW é simplesmente mais consistente e converge mais rapidamente para um ótimo local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNpeA0kXkEOA"
   },
   "source": [
    "# Conclusão\n",
    "\n",
    "AdamW parece ser um algoritmo mais consistente e que converge mais rapidamente para um ótimo local baseado na comparação de AdamW, SGD(lr=0,01), e SGD(lr=0,001)\n",
    "\n",
    "Isto significa que o melhor modelo teve um erro médio de 0,0225, em oposição ao erro médio mais baixo de 0,0207, que não é suficientemente significativo para justificar a inconsistência do otimizador SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3EYal1VkEOB"
   },
   "source": [
    "# Desafios\n",
    "\n",
    "Este conjunto de dados (ou pelo menos o conjunto de dados chessData.csv) tem mais de 16 milhões de posições e avaliações, o que torna bastante difícil trabalhar com ele. Por causa disso, tive que limitar a quantidade de dados realmente utilizada a 200.000 posições, 100.000 no conjunto de dados de treinamento e 100.000 no conjunto de dados de teste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFtEg1g4kEOB"
   },
   "source": [
    "# Referências\n",
    "\n",
    "1. https://www.kaggle.com/ronakbadhe/chess-evaluation-prediction\n",
    "2. https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation\n",
    "3. http://starship-knowledge.com/wp-content/uploads/2020/10/Perceptrons-1024x724.jpeg\n",
    "4. https://starship-knowledge.com/wp-content/uploads/2020/10/Perceptrons-1024x724.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GidrTuPLkEOB"
   },
   "source": [
    "# Contribution\n",
    "\n",
    "* Representation of castling rights, en passant, active color, halfmoves and fullmoves in an 8x8 grid\n",
    "* Neural Network Architecture\n",
    "* Testing AdamW vs SVG"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chess-position-evaluator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e3105560c81ae39e0834f5867d71e8b951b6beb57a3746bf5585c30fcba755d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
