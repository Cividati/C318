{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-QvR6AmkEN2"
      },
      "source": [
        "# Avaliador de Posições de Xadrez\n",
        "\n",
        "O xadrez tem sido uma referência clássica para inteligência artificial, aprendizado de máquinas e mineração de dados desde sua criação. Devido às simples regras determinísticas do xadrez aliadas às possibilidades exponenciais e difíceis de avaliar posições, ganhou o foco de muitos pesquisadores em Ciência da Computação. Hoje em dia, existem vários IAs de xadrez sofisticados que competem em um nível acima até mesmo do mais forte Grande Mestre de xadrez do mundo. Enquanto as máquinas de xadrez como Stockfish (do qual este conjunto de dados é baseado) e AlphaGo são muito complexas para tentar competir aqui, vamos ver se podemos usar as avaliações de Stockfish para construir uma função de avaliação de posição comparável.\n",
        "\n",
        "Para este projeto estarei usando a biblioteca de pytorch e construindo com ela uma Rede Neural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEBD3tVxksVF",
        "outputId": "890346a9-8851-4e25-8d2b-473acfb808f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hyeeXkQXkEN4"
      },
      "outputs": [],
      "source": [
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# for visualizing the results\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# for reading input data\n",
        "import pandas as pd\n",
        "\n",
        "# for parsing the FEN of chess positions\n",
        "import re\n",
        "\n",
        "# Measuring time in seconds:\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from datetime import timedelta\n",
        "import chess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAOmdyUIkEN5"
      },
      "source": [
        "Para representar uma posição de xadrez, é comum usar [Forsyth-Edwards Notation (FEN)](http://https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation) que contém todas as informações necessárias para reconstruir um jogo de xadrez a partir da posição atual. Para tornar estas informações utilizáveis para uma rede neural, usaremos um byte para representar se uma peça específica (torre branca, cavaleiro branco, etc...) está em um quadrado específico no tabuleiro de xadrez 8x8. Como há 6 peças diferentes e dois jogadores diferentes, isso significa que há 12 peças específicas que poderiam estar potencialmente em cada quadrado. \n",
        "\n",
        "No entanto, ainda precisamos manter um registro de informações como de quem é a vez, quais opções de castling ainda são legais, se en passant é possível, quantas meias jogadas desde uma jogada de peão ou captura de peças, e quantas jogadas o jogo já teve. Para fazer isso, usamos um tabuleiro adicional 8x8 onde as posições das torres representam direitos de castling, a 3ª e 6ª posições (fileira) mantêm registro de possíveis jogadas en passant, os e1 e e8 representam quem está em movimento, e a 4ª e 5ª posições representam o número de meias jogadas e jogadas completas como números binários (máximo possível é 255), respectivamente.\n",
        "\n",
        "Abaixo está uma função para fazer esta conversão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F-G2KNRQkEN6"
      },
      "outputs": [],
      "source": [
        "def fen_to_bit_vector(fen):\n",
        "    # piece placement - lowercase for black pieces, uppercase for white pieces. numbers represent consequtive spaces. / represents a new row \n",
        "    # active color - whose turn it is, either 'w' or 'b'\n",
        "    # castling rights - which castling moves are still legal K or k for kingside and Q or q for queenside, '-' if no legal castling moves for either player\n",
        "    # en passant - if the last move was a pawn moving up two squares, this is the space behind the square for the purposes of en passant\n",
        "    # halfmove clock - number of moves without a pawn move or piece capture, after 50 of which the game is a draw\n",
        "    # fullmove number - number of full turns starting at 1, increments after black's move\n",
        "\n",
        "    # Example FEN of starting position\n",
        "    # rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
        "    \n",
        "    parts = re.split(\" \", fen)\n",
        "    piece_placement = re.split(\"/\", parts[0])\n",
        "    active_color = parts[1]\n",
        "    castling_rights = parts[2]\n",
        "    en_passant = parts[3]\n",
        "    halfmove_clock = int(parts[4])\n",
        "    fullmove_clock = int(parts[5])\n",
        "\n",
        "    bit_vector = np.zeros((13, 8, 8), dtype=np.uint8)\n",
        "    \n",
        "    # piece to layer structure taken from reference [1]\n",
        "    piece_to_layer = {\n",
        "        'R': 1,\n",
        "        'N': 2,\n",
        "        'B': 3,\n",
        "        'Q': 4,\n",
        "        'K': 5,\n",
        "        'P': 6,\n",
        "        'p': 7,\n",
        "        'k': 8,\n",
        "        'q': 9,\n",
        "        'b': 10,\n",
        "        'n': 11,\n",
        "        'r': 12\n",
        "    }\n",
        "    \n",
        "    castling = {\n",
        "        'K': (7,7),\n",
        "        'Q': (7,0),\n",
        "        'k': (0,7),\n",
        "        'q': (0,0),\n",
        "    }\n",
        "\n",
        "    for r, row in enumerate(piece_placement):\n",
        "        c = 0\n",
        "        for piece in row:\n",
        "            if piece in piece_to_layer:\n",
        "                bit_vector[piece_to_layer[piece], r, c] = 1\n",
        "                c += 1\n",
        "            else:\n",
        "                c += int(piece)\n",
        "    \n",
        "    if en_passant != '-':\n",
        "        bit_vector[0, ord(en_passant[0]) - ord('a'), int(en_passant[1]) - 1] = 1\n",
        "    \n",
        "    if castling_rights != '-':\n",
        "        for char in castling_rights:\n",
        "            bit_vector[0, castling[char][0], castling[char][1]] = 1\n",
        "    \n",
        "    if active_color == 'w':\n",
        "        bit_vector[0, 7, 4] = 1\n",
        "    else:\n",
        "        bit_vector[0, 0, 4] = 1\n",
        "\n",
        "    if halfmove_clock > 0:\n",
        "        c = 7\n",
        "        while halfmove_clock > 0:\n",
        "            bit_vector[0, 3, c] = halfmove_clock%2\n",
        "            halfmove_clock = halfmove_clock // 2\n",
        "            c -= 1\n",
        "            if c < 0:\n",
        "                break\n",
        "\n",
        "    if fullmove_clock > 0:\n",
        "        c = 7\n",
        "        while fullmove_clock > 0:\n",
        "            bit_vector[0, 4, c] = fullmove_clock%2\n",
        "            fullmove_clock = fullmove_clock // 2\n",
        "            c -= 1\n",
        "            if c < 0:\n",
        "                break\n",
        "\n",
        "    return bit_vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76SVquvLkEN7",
        "outputId": "f4f58f4b-7405-4bf6-ddbc-92a68fc81627"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>r n b q k b n r\np p p p p p p p\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\n. . . . . . . .\nP P P P P P P P\nR N B Q K B N R</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-bishop\" class=\"white bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#fff\" stroke-linecap=\"butt\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zM15 32c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" /></g><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke-linejoin=\"miter\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-knight\" class=\"black knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#ececec; stroke:#ececec;\" /><path d=\"M 24.55,10.4 L 24.1,11.85 L 24.6,12 C 27.75,13 30.25,14.49 32.5,18.75 C 34.75,23.01 35.75,29.06 35.25,39 L 35.2,39.5 L 37.45,39.5 L 37.5,39 C 38,28.94 36.62,22.15 34.25,17.66 C 31.88,13.17 28.46,11.02 25.06,10.5 L 24.55,10.4 z \" style=\"fill:#ececec; stroke:none;\" /></g><g id=\"black-bishop\" class=\"black bishop\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 36c3.39-.97 10.11.43 13.5-2 3.39 2.43 10.11 1.03 13.5 2 0 0 1.65.54 3 2-.68.97-1.65.99-3 .5-3.39-.97-10.11.46-13.5-1-3.39 1.46-10.11.03-13.5 1-1.354.49-2.323.47-3-.5 1.354-1.94 3-2 3-2zm6-4c2.5 2.5 12.5 2.5 15 0 .5-1.5 0-2 0-2 0-2.5-2.5-4-2.5-4 5.5-1.5 6-11.5-5-15.5-11 4-10.5 14-5 15.5 0 0-2.5 1.5-2.5 4 0 0-.5.5 0 2zM25 8a2.5 2.5 0 1 1-5 0 2.5 2.5 0 1 1 5 0z\" fill=\"#000\" stroke-linecap=\"butt\" /><path d=\"M17.5 26h10M15 30h15m-7.5-14.5v5M20 18h5\" stroke=\"#fff\" stroke-linejoin=\"miter\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-queen\" class=\"black queen\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><g fill=\"#000\" stroke=\"none\"><circle cx=\"6\" cy=\"12\" r=\"2.75\" /><circle cx=\"14\" cy=\"9\" r=\"2.75\" /><circle cx=\"22.5\" cy=\"8\" r=\"2.75\" /><circle cx=\"31\" cy=\"9\" r=\"2.75\" /><circle cx=\"39\" cy=\"12\" r=\"2.75\" /></g><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2.5-12.5L31 25l-.3-14.1-5.2 13.6-3-14.5-3 14.5-5.2-13.6L14 25 6.5 13.5 9 26zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11 38.5a35 35 1 0 0 23 0\" fill=\"none\" stroke-linecap=\"butt\" /><path d=\"M11 29a35 35 1 0 1 23 0M12.5 31.5h20M11.5 34.5a35 35 1 0 0 22 0M10.5 37.5a35 35 1 0 0 24 0\" fill=\"none\" stroke=\"#fff\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(60, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(105, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(150, 330)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(195, 330)\" /><use href=\"#white-bishop\" xlink:href=\"#white-bishop\" transform=\"translate(240, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(285, 330)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(330, 330)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(60, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(195, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(285, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(330, 285)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(15, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(150, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(195, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(240, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(285, 60)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(330, 60)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(60, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(105, 15)\" /><use href=\"#black-queen\" xlink:href=\"#black-queen\" transform=\"translate(150, 15)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(195, 15)\" /><use href=\"#black-bishop\" xlink:href=\"#black-bishop\" transform=\"translate(240, 15)\" /><use href=\"#black-knight\" xlink:href=\"#black-knight\" transform=\"translate(285, 15)\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(330, 15)\" /></svg>",
            "text/plain": [
              "Board('rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
        "\n",
        "chess.Board(fen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JCSr6-wkEN8"
      },
      "source": [
        "O primeiro tabuleiro 8x8 (0º índice) contém todas as informações extras e os 12 tabuleiros seguintes (1 a 12) representam a localização das peças na ordem \n",
        "\n",
        "1. Torre branca\n",
        "2. Cavaleiro Branco\n",
        "3. Bispo Branco\n",
        "4. Rainha Branca\n",
        "5. Rei Branco\n",
        "6. Peão Branco\n",
        "7. Peão Preto\n",
        "8. Rei Negro\n",
        "9. Rainha Negra\n",
        "10. Bispo negro\n",
        "11. Cavaleiro Negro\n",
        "12. Torre Negra\n",
        "\n",
        "Observe como as peças se alinham corretamente com a posição inicial com o primeiro tabuleiro indicando corretamente que está na vez da branca se mover."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf5IjQEKkEN8"
      },
      "source": [
        "# Rede Neural\n",
        "\n",
        "Começaremos com uma simples Rede Neural Feed-Forward que está totalmente conectada. As Redes Neurais são nomeadas por sua estrutura ser análoga à dos neurônios no cérebro humano. A idéia é que, no cérebro humano, quando um neurônio recebe um impulso elétrico através de suas sinapses, ele às vezes dispara um impulso elétrico para outros neurônios, criando uma reação em cadeia. Para as redes neurais, nossos neurônios são nós, nossas sinapses são bordas (com pesos correspondentes) e a queima do neurônio é a função de ativação e saída do nó. \n",
        "\n",
        "![Perceptron](http://starship-knowledge.com/wp-content/uploads/2020/10/Perceptrons-1024x724.jpeg)\n",
        "\n",
        "O objetivo da Rede Neural é ter pesos tais que, após todas as reações em cadeia dos nós que recebem entradas e produzem saídas, a saída de informações do nó final represente a avaliação da posição do xadrez que iniciou o processo. Para encontrar realmente tais pesos, usaremos um método conhecido como retropropagação, que ajusta iterativamente os pesos na rede para aproximar a saída da resposta que desejamos.\n",
        "\n",
        "Tecnicamente falando, para cada registro de treinamento (um FEN e uma avaliação) inserimos a posição na Rede Neural, e após obter um resultado, calculamos o erro entre o resultado e a avaliação correta que pode ser representada como uma função de erro. Para alterar os pesos de forma a minimizar esta função de erro, calculamos o gradiente da função de erro e ajustamos os pesos na direção oposta. Isto significa que, se excedermos, queremos diminuir nossa avaliação e, se não conseguirmos, queremos aumentar nossa avaliação.\n",
        "\n",
        "![Gradiente de descida](https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_gradient_descent_1.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4UUqb2VIkEN9"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(832, 832)\n",
        "        self.fc2 = nn.Linear(832, 416)\n",
        "        self.fc3 = nn.Linear(416, 208)\n",
        "        self.fc4 = nn.Linear(208, 104)\n",
        "        self.fc5 = nn.Linear(104, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "brRrxjBBkEN9"
      },
      "outputs": [],
      "source": [
        "# ChessDataset code and eval_to_int code taken from reference [1]\n",
        "class ChessDataset(Dataset):\n",
        "    def __init__(self, data_frame):\n",
        "        self.fens = torch.from_numpy(np.array([*map(fen_to_bit_vector, data_frame[\"FEN\"])], dtype=np.float32))\n",
        "        self.evals = torch.Tensor([[x] for x in data_frame[\"Evaluation\"]])\n",
        "        self._len = len(self.evals)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._len\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.fens[index], self.evals[index]\n",
        "\n",
        "\n",
        "def eval_to_int(evaluation):\n",
        "    try:\n",
        "        res = int(evaluation)\n",
        "    except ValueError:\n",
        "        res = 10000 if evaluation[1] == '+' else -10000\n",
        "    return res / 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jU5Ab8TGkEN-"
      },
      "outputs": [],
      "source": [
        "def AdamW_main(trainset, testset, batch_size, epochs, device):\n",
        "    print(\"Converting to pytorch Dataset...\")\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    net = Net().to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(net.parameters())\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            # if i == len(trainloader):    # print every 2000 mini-batches\n",
        "            #     # denominator for loss should represent the number of positions evaluated \n",
        "            #     # independent of the batch size\n",
        "            #     print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (1000*len(labels))))\n",
        "            #     running_loss = 0.0\n",
        "\n",
        "            # print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (1000*len(labels))))\n",
        "        print('epoch: %d - running loss: %.5f'% (epoch, running_loss/len(trainset)))\n",
        "        running_loss = 0.0\n",
        "            \n",
        "    print('Finished Training')\n",
        "\n",
        "    # PATH = './chess.pth'\n",
        "    # torch.save(net.state_dict(), PATH)\n",
        "\n",
        "    print('Evaluating model')\n",
        "\n",
        "    count = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n",
        "            \n",
        "            # count should represent the number of positions evaluated \n",
        "            # independent of the batch size\n",
        "            count += len(labels)\n",
        "         \n",
        "            # if count % 10000 == 0:\n",
        "            #     print('Average error of the model on the {} tactics positions is {}'.format(count, loss/count))\n",
        "    print('Average error of the AdamW model on the tactics positions is %.5f'%(loss/count))\n",
        "    return 'Average error of the AdamW model on the tactics positions is %.5f'%(loss/count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JdXc0GqCkEN_"
      },
      "outputs": [],
      "source": [
        "def SGD_main(trainset, testset, batch_size, epochs, device, learning_rate):\n",
        "\n",
        "    print(\"Converting to pytorch Dataset...\")\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    net = Net().to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr = learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            # if i % 2000 == 0:    # print every 1000 mini-batches\n",
        "            #     # denominator for loss should represent the number of positions evaluated \n",
        "            #     # independent of the batch size\n",
        "            #     print('[%d, %d] loss: %.3f' % (epoch + 1, i + 1, running_loss / (1000*len(labels))))\n",
        "            #     running_loss = 0.0\n",
        "        print('epoch: %d - running loss: %.5f' % (epoch, running_loss/len(trainset)))\n",
        "        running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # PATH = './chess.pth'\n",
        "    # torch.save(net.state_dict(), PATH)\n",
        "\n",
        "    print('Evaluating model')\n",
        "\n",
        "    count = 0\n",
        "    total_loss = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            #print(\"Correct eval: {}, Predicted eval: {}, loss: {}\".format(labels, outputs, loss))\n",
        "            \n",
        "            # count should represent the number of positions evaluated \n",
        "            # independent of the batch size\n",
        "            count += len(labels)\n",
        "            total_loss += loss\n",
        "            # if count % 10000 == 0:\n",
        "            #     print('Average error of the model on the {} tactics positions is {}'.format(count, loss/count))\n",
        "    print('Average error of the SGD model - %.3f is %.5f' %(learning_rate, loss/count))\n",
        "    return 'Average error of the SGD model - %.3f is %.5f'%(learning_rate, loss/count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_models(MAX_DATA, FILE_PATH = './', BATCH_SIZE = 10, EPOCHS = 10, DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "\n",
        "    # GLOBAL VARS\n",
        "    # FILE_PATH='./'\n",
        "    # MAX_DATA = 100000\n",
        "    # BATCH_SIZE = 10\n",
        "    # EPOCHS = 10\n",
        "    # DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # TODO: train test split sklearn para pegar dados aleatórios\n",
        "    \n",
        "    print(\"Using device {}\".format(DEVICE))\n",
        "\n",
        "    print(\"Preparing Training Data...\")\n",
        "    train_data = pd.read_csv(FILE_PATH + \"chessData.csv\")\n",
        "    # print(\"total chessData size:\",train_data.shape[0]/10**6,\"m (\", train_data.shape[0],\")\")\n",
        "    train_data = train_data[:MAX_DATA]\n",
        "    train_data[\"Evaluation\"] = train_data[\"Evaluation\"].map(eval_to_int)\n",
        "    trainset = ChessDataset(train_data)\n",
        "\n",
        "    print(\"Preparing Test Data...\")\n",
        "    test_data = pd.read_csv(FILE_PATH + \"tactic_evals.csv\")\n",
        "    # print(\"total: tactic_evals size:\",test_data.shape[0]//10**6,\"m(\", test_data.shape[0],\")\")\n",
        "    test_data = test_data[:MAX_DATA]\n",
        "    test_data[\"Evaluation\"] = test_data[\"Evaluation\"].map(eval_to_int)\n",
        "    testset = ChessDataset(test_data)\n",
        "\n",
        "    print(\"total data: {}\".format(len(trainset)))\n",
        "    print(\"-\"*50)\n",
        "    \n",
        "    print(\"running AdamW\")\n",
        "    start = timer()\n",
        "    AdamW = AdamW_main(trainset, testset, BATCH_SIZE, EPOCHS, DEVICE)\n",
        "    end = timer()\n",
        "    print('time elapsed on AdamW:', timedelta(seconds=end-start))\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    print(\"running SGD - 0.01\")\n",
        "    start = timer()\n",
        "    SGD_01 = SGD_main(trainset, testset, BATCH_SIZE, EPOCHS, DEVICE, 0.01)\n",
        "    end = timer()\n",
        "    print('time elapsed on SGD - 0.01:', timedelta(seconds=end-start))\n",
        "\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    print(\"running SGD - 0.001\")\n",
        "    start = timer()\n",
        "    SGD_001 = SGD_main(trainset, testset, BATCH_SIZE, EPOCHS, DEVICE, 0.001)\n",
        "    end = timer()\n",
        "    print('time elapsed on SGD - 0.001:', timedelta(seconds=end-start))\n",
        "\n",
        "    print(\"-\"*50)\n",
        "    print(\"AdamW: \", AdamW) \n",
        "    print(\"SGD - 0.01: \", SGD_01) \n",
        "    print(\"SGD - 0.001: \", SGD_001)\n",
        "    \n",
        "    return (AdamW, SGD_01, SGD_001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "Preparing Training Data...\n",
            "Preparing Test Data...\n",
            "total data: 10000\n",
            "--------------------------------------------------\n",
            "running AdamW\n",
            "Converting to pytorch Dataset...\n",
            "epoch: 0 - running loss: 7.12897\n",
            "epoch: 1 - running loss: 3.50771\n",
            "epoch: 2 - running loss: 2.35126\n",
            "epoch: 3 - running loss: 1.55557\n",
            "epoch: 4 - running loss: 1.31366\n",
            "epoch: 5 - running loss: 1.14532\n",
            "epoch: 6 - running loss: 1.25080\n",
            "epoch: 7 - running loss: 1.01885\n",
            "epoch: 8 - running loss: 0.71597\n",
            "epoch: 9 - running loss: 0.70916\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the AdamW model on the tactics positions is 0.10275\n",
            "time elapsed on AdamW: 0:00:26.282225\n",
            "--------------------------------------------------\n",
            "running SGD - 0.01\n",
            "Converting to pytorch Dataset...\n",
            "epoch: 0 - running loss: nan\n",
            "epoch: 1 - running loss: nan\n",
            "epoch: 2 - running loss: nan\n",
            "epoch: 3 - running loss: nan\n",
            "epoch: 4 - running loss: nan\n",
            "epoch: 5 - running loss: nan\n",
            "epoch: 6 - running loss: nan\n",
            "epoch: 7 - running loss: nan\n",
            "epoch: 8 - running loss: nan\n",
            "epoch: 9 - running loss: nan\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the SGD model - 0.010 is nan\n",
            "time elapsed on SGD - 0.01: 0:00:17.696170\n",
            "--------------------------------------------------\n",
            "running SGD - 0.001\n",
            "Converting to pytorch Dataset...\n",
            "epoch: 0 - running loss: 10.54481\n",
            "epoch: 1 - running loss: 10.54267\n",
            "epoch: 2 - running loss: 10.53801\n",
            "epoch: 3 - running loss: 10.47012\n",
            "epoch: 4 - running loss: 8.58264\n",
            "epoch: 5 - running loss: 5.59405\n",
            "epoch: 6 - running loss: 4.44558\n",
            "epoch: 7 - running loss: 4.28962\n",
            "epoch: 8 - running loss: 3.33625\n",
            "epoch: 9 - running loss: 3.03382\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the SGD model - 0.001 is 0.09725\n",
            "time elapsed on SGD - 0.001: 0:00:22.681056\n",
            "--------------------------------------------------\n",
            "AdamW:  Average error of the AdamW model on the tactics positions is 0.10275\n",
            "SGD - 0.01:  Average error of the SGD model - 0.010 is nan\n",
            "SGD - 0.001:  Average error of the SGD model - 0.001 is 0.09725\n"
          ]
        }
      ],
      "source": [
        "models_10k = run_models(10_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using 100k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "Preparing Training Data...\n",
            "Preparing Test Data...\n",
            "total data: 100000\n",
            "--------------------------------------------------\n",
            "running AdamW\n",
            "Converting to pytorch Dataset...\n",
            "epoch: 0 - running loss: 6.22598\n",
            "epoch: 1 - running loss: 3.10262\n",
            "epoch: 2 - running loss: 2.29878\n",
            "epoch: 3 - running loss: 1.82177\n",
            "epoch: 4 - running loss: 1.58512\n",
            "epoch: 5 - running loss: 1.40528\n",
            "epoch: 6 - running loss: 1.24684\n",
            "epoch: 7 - running loss: 1.08620\n",
            "epoch: 8 - running loss: 1.11725\n",
            "epoch: 9 - running loss: 0.98872\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the AdamW model on the tactics positions is 0.02395\n",
            "time elapsed on AdamW: 0:03:54.697437\n",
            "--------------------------------------------------\n",
            "running SGD - 0.01\n",
            "Converting to pytorch Dataset...\n",
            "epoch: 0 - running loss: 9.36998\n",
            "epoch: 1 - running loss: nan\n",
            "epoch: 2 - running loss: nan\n",
            "epoch: 3 - running loss: nan\n",
            "epoch: 4 - running loss: nan\n",
            "epoch: 5 - running loss: nan\n",
            "epoch: 6 - running loss: nan\n",
            "epoch: 7 - running loss: nan\n",
            "epoch: 8 - running loss: nan\n",
            "epoch: 9 - running loss: nan\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the SGD model - 0.010 is nan\n",
            "time elapsed on SGD - 0.01: 0:02:44.141937\n",
            "--------------------------------------------------\n",
            "running SGD - 0.001\n",
            "Converting to pytorch Dataset...\n",
            "epoch: 0 - running loss: 9.36000\n",
            "epoch: 1 - running loss: 8.72790\n",
            "epoch: 2 - running loss: 7.92471\n",
            "epoch: 3 - running loss: 7.37294\n",
            "epoch: 4 - running loss: 6.50655\n",
            "epoch: 5 - running loss: 6.77526\n",
            "epoch: 6 - running loss: 7.61482\n"
          ]
        }
      ],
      "source": [
        "models_100k = run_models(100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "Preparing Training Data...\n",
            "Preparing Test Data...\n",
            "total data: 200000\n",
            "--------------------------------------------------\n",
            "running AdamW\n",
            "Converting to pytorch Dataset...\n",
            "epoch: 0 - running loss: 6.23384\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Rubens\\Documents\\repos\\C318\\chess_position_evaluator.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000016?line=0'>1</a>\u001b[0m models_200k \u001b[39m=\u001b[39m run_models(\u001b[39m200_000\u001b[39;49m)\n",
            "\u001b[1;32mc:\\Users\\Rubens\\Documents\\repos\\C318\\chess_position_evaluator.ipynb Cell 13'\u001b[0m in \u001b[0;36mrun_models\u001b[1;34m(MAX_DATA, FILE_PATH, BATCH_SIZE, EPOCHS, DEVICE)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000012?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mrunning AdamW\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000012?line=29'>30</a>\u001b[0m start \u001b[39m=\u001b[39m timer()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000012?line=30'>31</a>\u001b[0m AdamW \u001b[39m=\u001b[39m AdamW_main(trainset, testset, BATCH_SIZE, EPOCHS, DEVICE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000012?line=31'>32</a>\u001b[0m end \u001b[39m=\u001b[39m timer()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000012?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtime elapsed on AdamW:\u001b[39m\u001b[39m'\u001b[39m, timedelta(seconds\u001b[39m=\u001b[39mend\u001b[39m-\u001b[39mstart))\n",
            "\u001b[1;32mc:\\Users\\Rubens\\Documents\\repos\\C318\\chess_position_evaluator.ipynb Cell 11'\u001b[0m in \u001b[0;36mAdamW_main\u001b[1;34m(trainset, testset, batch_size, epochs, device)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000010?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000010?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000010?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000010?line=28'>29</a>\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Rubens/Documents/repos/C318/chess_position_evaluator.ipynb#ch0000010?line=29'>30</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\Rubens\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Rubens\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Rubens\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:145\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 145\u001b[0m     F\u001b[39m.\u001b[39;49madamw(params_with_grad,\n\u001b[0;32m    146\u001b[0m             grads,\n\u001b[0;32m    147\u001b[0m             exp_avgs,\n\u001b[0;32m    148\u001b[0m             exp_avg_sqs,\n\u001b[0;32m    149\u001b[0m             max_exp_avg_sqs,\n\u001b[0;32m    150\u001b[0m             state_steps,\n\u001b[0;32m    151\u001b[0m             amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    152\u001b[0m             beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    153\u001b[0m             beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    154\u001b[0m             lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m             weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m             eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m             maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Users\\Rubens\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\_functional.py:143\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    140\u001b[0m bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m step\n\u001b[0;32m    142\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m    144\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[0;32m    146\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "models_200k = run_models(200_000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "Preparing Training Data...\n",
            "Preparing Test Data...\n",
            "total data: 300000\n",
            "--------------------------------------------------\n",
            "running AdamW\n",
            "Converting to pytorch Dataset...\n",
            "[1,     1] loss: 0.001\n",
            "[1,  1001] loss: 12.757\n",
            "[1,  2001] loss: 12.158\n",
            "[1,  3001] loss: 10.715\n",
            "[1,  4001] loss: 11.585\n",
            "[1,  5001] loss: 10.389\n",
            "[1,  6001] loss: 10.372\n",
            "[1,  7001] loss: 9.649\n",
            "[1,  8001] loss: 8.385\n",
            "[1,  9001] loss: 7.608\n",
            "[1, 10001] loss: 9.452\n",
            "[1, 11001] loss: 8.511\n",
            "[1, 12001] loss: 8.803\n",
            "[1, 13001] loss: 8.305\n",
            "[1, 14001] loss: 8.764\n",
            "[1, 15001] loss: 7.129\n",
            "[1, 16001] loss: 7.118\n",
            "[1, 17001] loss: 6.661\n",
            "[1, 18001] loss: 6.906\n",
            "[1, 19001] loss: 7.396\n",
            "[1, 20001] loss: 7.883\n",
            "[1, 21001] loss: 7.469\n",
            "[1, 22001] loss: 6.965\n",
            "[1, 23001] loss: 6.112\n",
            "[1, 24001] loss: 6.865\n",
            "[1, 25001] loss: 6.741\n",
            "[1, 26001] loss: 6.919\n",
            "[1, 27001] loss: 6.591\n",
            "[1, 28001] loss: 6.892\n",
            "[1, 29001] loss: 6.697\n",
            "[2,     1] loss: 0.000\n",
            "[2,  1001] loss: 5.630\n",
            "[2,  2001] loss: 4.912\n",
            "[2,  3001] loss: 4.744\n",
            "[2,  4001] loss: 5.026\n",
            "[2,  5001] loss: 4.609\n",
            "[2,  6001] loss: 4.803\n",
            "[2,  7001] loss: 4.535\n",
            "[2,  8001] loss: 5.060\n",
            "[2,  9001] loss: 5.444\n",
            "[2, 10001] loss: 3.876\n",
            "[2, 11001] loss: 4.867\n",
            "[2, 12001] loss: 4.953\n",
            "[2, 13001] loss: 4.128\n",
            "[2, 14001] loss: 4.920\n",
            "[2, 15001] loss: 4.692\n",
            "[2, 16001] loss: 5.003\n",
            "[2, 17001] loss: 5.456\n",
            "[2, 18001] loss: 4.388\n",
            "[2, 19001] loss: 5.281\n",
            "[2, 20001] loss: 4.726\n",
            "[2, 21001] loss: 4.435\n",
            "[2, 22001] loss: 4.876\n",
            "[2, 23001] loss: 4.379\n",
            "[2, 24001] loss: 4.239\n",
            "[2, 25001] loss: 4.260\n",
            "[2, 26001] loss: 4.340\n",
            "[2, 27001] loss: 4.533\n",
            "[2, 28001] loss: 4.154\n",
            "[2, 29001] loss: 5.384\n",
            "[3,     1] loss: 0.000\n",
            "[3,  1001] loss: 4.105\n",
            "[3,  2001] loss: 3.494\n",
            "[3,  3001] loss: 3.704\n",
            "[3,  4001] loss: 4.270\n",
            "[3,  5001] loss: 3.547\n",
            "[3,  6001] loss: 3.059\n",
            "[3,  7001] loss: 3.593\n",
            "[3,  8001] loss: 4.391\n",
            "[3,  9001] loss: 3.759\n",
            "[3, 10001] loss: 3.888\n",
            "[3, 11001] loss: 3.818\n",
            "[3, 12001] loss: 3.493\n",
            "[3, 13001] loss: 3.693\n",
            "[3, 14001] loss: 4.506\n",
            "[3, 15001] loss: 3.363\n",
            "[3, 16001] loss: 3.533\n",
            "[3, 17001] loss: 3.596\n",
            "[3, 18001] loss: 3.385\n",
            "[3, 19001] loss: 3.939\n",
            "[3, 20001] loss: 3.868\n",
            "[3, 21001] loss: 3.828\n",
            "[3, 22001] loss: 3.538\n",
            "[3, 23001] loss: 4.052\n",
            "[3, 24001] loss: 3.418\n",
            "[3, 25001] loss: 3.317\n",
            "[3, 26001] loss: 4.525\n",
            "[3, 27001] loss: 3.332\n",
            "[3, 28001] loss: 4.029\n",
            "[3, 29001] loss: 4.087\n",
            "[4,     1] loss: 0.000\n",
            "[4,  1001] loss: 2.977\n",
            "[4,  2001] loss: 2.741\n",
            "[4,  3001] loss: 3.013\n",
            "[4,  4001] loss: 3.051\n",
            "[4,  5001] loss: 3.106\n",
            "[4,  6001] loss: 3.686\n",
            "[4,  7001] loss: 3.572\n",
            "[4,  8001] loss: 3.166\n",
            "[4,  9001] loss: 3.090\n",
            "[4, 10001] loss: 3.380\n",
            "[4, 11001] loss: 3.226\n",
            "[4, 12001] loss: 3.363\n",
            "[4, 13001] loss: 3.244\n",
            "[4, 14001] loss: 3.967\n",
            "[4, 15001] loss: 4.006\n",
            "[4, 16001] loss: 3.527\n",
            "[4, 17001] loss: 3.251\n",
            "[4, 18001] loss: 3.058\n",
            "[4, 19001] loss: 3.954\n",
            "[4, 20001] loss: 3.374\n",
            "[4, 21001] loss: 2.975\n",
            "[4, 22001] loss: 2.936\n",
            "[4, 23001] loss: 3.854\n",
            "[4, 24001] loss: 3.149\n",
            "[4, 25001] loss: 3.748\n",
            "[4, 26001] loss: 3.332\n",
            "[4, 27001] loss: 3.290\n",
            "[4, 28001] loss: 3.507\n",
            "[4, 29001] loss: 3.866\n",
            "[5,     1] loss: 0.000\n",
            "[5,  1001] loss: 2.369\n",
            "[5,  2001] loss: 2.607\n",
            "[5,  3001] loss: 2.762\n",
            "[5,  4001] loss: 2.993\n",
            "[5,  5001] loss: 2.849\n",
            "[5,  6001] loss: 3.817\n",
            "[5,  7001] loss: 3.054\n",
            "[5,  8001] loss: 3.398\n",
            "[5,  9001] loss: 3.298\n",
            "[5, 10001] loss: 2.609\n",
            "[5, 11001] loss: 3.006\n",
            "[5, 12001] loss: 3.052\n",
            "[5, 13001] loss: 4.435\n",
            "[5, 14001] loss: 2.690\n",
            "[5, 15001] loss: 3.169\n",
            "[5, 16001] loss: 3.287\n",
            "[5, 17001] loss: 3.377\n",
            "[5, 18001] loss: 3.005\n",
            "[5, 19001] loss: 3.077\n",
            "[5, 20001] loss: 3.317\n",
            "[5, 21001] loss: 3.207\n",
            "[5, 22001] loss: 3.323\n",
            "[5, 23001] loss: 2.843\n",
            "[5, 24001] loss: 3.661\n",
            "[5, 25001] loss: 3.365\n",
            "[5, 26001] loss: 3.390\n",
            "[5, 27001] loss: 3.691\n",
            "[5, 28001] loss: 3.107\n",
            "[5, 29001] loss: 3.124\n",
            "[6,     1] loss: 0.000\n",
            "[6,  1001] loss: 2.215\n",
            "[6,  2001] loss: 2.354\n",
            "[6,  3001] loss: 2.452\n",
            "[6,  4001] loss: 2.650\n",
            "[6,  5001] loss: 2.778\n",
            "[6,  6001] loss: 2.456\n",
            "[6,  7001] loss: 2.608\n",
            "[6,  8001] loss: 2.669\n",
            "[6,  9001] loss: 2.715\n",
            "[6, 10001] loss: 2.366\n",
            "[6, 11001] loss: 3.322\n",
            "[6, 12001] loss: 2.461\n",
            "[6, 13001] loss: 3.040\n",
            "[6, 14001] loss: 3.263\n",
            "[6, 15001] loss: 3.065\n",
            "[6, 16001] loss: 2.623\n",
            "[6, 17001] loss: 2.500\n",
            "[6, 18001] loss: 3.175\n",
            "[6, 19001] loss: 3.414\n",
            "[6, 20001] loss: 2.994\n",
            "[6, 21001] loss: 2.948\n",
            "[6, 22001] loss: 3.967\n",
            "[6, 23001] loss: 3.359\n",
            "[6, 24001] loss: 3.475\n",
            "[6, 25001] loss: 3.115\n",
            "[6, 26001] loss: 3.118\n",
            "[6, 27001] loss: 3.255\n",
            "[6, 28001] loss: 3.091\n",
            "[6, 29001] loss: 2.888\n",
            "[7,     1] loss: 0.003\n",
            "[7,  1001] loss: 2.167\n",
            "[7,  2001] loss: 2.240\n",
            "[7,  3001] loss: 3.083\n",
            "[7,  4001] loss: 2.039\n",
            "[7,  5001] loss: 2.257\n",
            "[7,  6001] loss: 2.952\n",
            "[7,  7001] loss: 2.367\n",
            "[7,  8001] loss: 2.529\n",
            "[7,  9001] loss: 2.677\n",
            "[7, 10001] loss: 2.816\n",
            "[7, 11001] loss: 2.906\n",
            "[7, 12001] loss: 3.436\n",
            "[7, 13001] loss: 3.471\n",
            "[7, 14001] loss: 2.993\n",
            "[7, 15001] loss: 2.596\n",
            "[7, 16001] loss: 2.576\n",
            "[7, 17001] loss: 2.404\n",
            "[7, 18001] loss: 2.626\n",
            "[7, 19001] loss: 2.813\n",
            "[7, 20001] loss: 2.913\n",
            "[7, 21001] loss: 2.788\n",
            "[7, 22001] loss: 3.122\n",
            "[7, 23001] loss: 2.748\n",
            "[7, 24001] loss: 2.885\n",
            "[7, 25001] loss: 3.352\n",
            "[7, 26001] loss: 3.217\n",
            "[7, 27001] loss: 2.502\n",
            "[7, 28001] loss: 2.875\n",
            "[7, 29001] loss: 3.681\n",
            "[8,     1] loss: 0.001\n",
            "[8,  1001] loss: 2.390\n",
            "[8,  2001] loss: 2.245\n",
            "[8,  3001] loss: 2.581\n",
            "[8,  4001] loss: 2.643\n",
            "[8,  5001] loss: 2.490\n",
            "[8,  6001] loss: 2.377\n",
            "[8,  7001] loss: 2.761\n",
            "[8,  8001] loss: 2.371\n",
            "[8,  9001] loss: 2.591\n",
            "[8, 10001] loss: 2.740\n",
            "[8, 11001] loss: 2.845\n",
            "[8, 12001] loss: 2.884\n",
            "[8, 13001] loss: 2.804\n",
            "[8, 14001] loss: 2.853\n",
            "[8, 15001] loss: 3.037\n",
            "[8, 16001] loss: 2.540\n",
            "[8, 17001] loss: 2.638\n",
            "[8, 18001] loss: 2.980\n",
            "[8, 19001] loss: 2.701\n",
            "[8, 20001] loss: 2.717\n",
            "[8, 21001] loss: 2.661\n",
            "[8, 22001] loss: 3.122\n",
            "[8, 23001] loss: 2.682\n",
            "[8, 24001] loss: 3.218\n",
            "[8, 25001] loss: 3.355\n",
            "[8, 26001] loss: 2.229\n",
            "[8, 27001] loss: 2.797\n",
            "[8, 28001] loss: 3.799\n",
            "[8, 29001] loss: 2.984\n",
            "[9,     1] loss: 0.000\n",
            "[9,  1001] loss: 2.222\n",
            "[9,  2001] loss: 2.298\n",
            "[9,  3001] loss: 2.326\n",
            "[9,  4001] loss: 2.577\n",
            "[9,  5001] loss: 1.855\n",
            "[9,  6001] loss: 2.512\n",
            "[9,  7001] loss: 2.459\n",
            "[9,  8001] loss: 2.357\n",
            "[9,  9001] loss: 2.485\n",
            "[9, 10001] loss: 2.753\n",
            "[9, 11001] loss: 2.547\n",
            "[9, 12001] loss: 2.537\n",
            "[9, 13001] loss: 3.020\n",
            "[9, 14001] loss: 2.597\n",
            "[9, 15001] loss: 2.810\n",
            "[9, 16001] loss: 2.587\n",
            "[9, 17001] loss: 2.531\n",
            "[9, 18001] loss: 2.550\n",
            "[9, 19001] loss: 3.089\n",
            "[9, 20001] loss: 3.273\n",
            "[9, 21001] loss: 3.464\n",
            "[9, 22001] loss: 2.996\n",
            "[9, 23001] loss: 3.478\n",
            "[9, 24001] loss: 3.342\n",
            "[9, 25001] loss: 2.609\n",
            "[9, 26001] loss: 3.033\n",
            "[9, 27001] loss: 3.488\n",
            "[9, 28001] loss: 2.512\n",
            "[9, 29001] loss: 2.979\n",
            "[10,     1] loss: 0.001\n",
            "[10,  1001] loss: 2.036\n",
            "[10,  2001] loss: 2.010\n",
            "[10,  3001] loss: 2.405\n",
            "[10,  4001] loss: 2.191\n",
            "[10,  5001] loss: 1.956\n",
            "[10,  6001] loss: 2.274\n",
            "[10,  7001] loss: 2.273\n",
            "[10,  8001] loss: 3.058\n",
            "[10,  9001] loss: 2.246\n",
            "[10, 10001] loss: 2.469\n",
            "[10, 11001] loss: 2.586\n",
            "[10, 12001] loss: 2.313\n",
            "[10, 13001] loss: 2.689\n",
            "[10, 14001] loss: 2.593\n",
            "[10, 15001] loss: 2.552\n",
            "[10, 16001] loss: 2.743\n",
            "[10, 17001] loss: 2.802\n",
            "[10, 18001] loss: 2.482\n",
            "[10, 19001] loss: 2.453\n",
            "[10, 20001] loss: 2.835\n",
            "[10, 21001] loss: 2.611\n",
            "[10, 22001] loss: 3.110\n",
            "[10, 23001] loss: 3.518\n",
            "[10, 24001] loss: 3.133\n",
            "[10, 25001] loss: 3.300\n",
            "[10, 26001] loss: 3.151\n",
            "[10, 27001] loss: 3.485\n",
            "[10, 28001] loss: 2.924\n",
            "[10, 29001] loss: 2.595\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the model on the 10000 tactics positions is 0.10255458950996399\n",
            "Average error of the model on the 20000 tactics positions is 0.002134667243808508\n",
            "Average error of the model on the 30000 tactics positions is 0.000674869806971401\n",
            "Average error of the model on the 40000 tactics positions is 0.00041468723793514073\n",
            "Average error of the model on the 50000 tactics positions is 0.0006353530334308743\n",
            "Average error of the model on the 60000 tactics positions is 0.0008269500103779137\n",
            "Average error of the model on the 70000 tactics positions is 0.057510919868946075\n",
            "Average error of the model on the 80000 tactics positions is 0.020874181762337685\n",
            "Average error of the model on the 90000 tactics positions is 0.023289846256375313\n",
            "Average error of the model on the 100000 tactics positions is 0.02181357331573963\n",
            "Average error of the model on the 110000 tactics positions is 0.03602655977010727\n",
            "Average error of the model on the 120000 tactics positions is 0.00037566866376437247\n",
            "Average error of the model on the 130000 tactics positions is 0.04917759448289871\n",
            "Average error of the model on the 140000 tactics positions is 0.04167325794696808\n",
            "Average error of the model on the 150000 tactics positions is 0.01693890430033207\n",
            "Average error of the model on the 160000 tactics positions is 0.019120490178465843\n",
            "Average error of the model on the 170000 tactics positions is 0.017422350123524666\n",
            "Average error of the model on the 180000 tactics positions is 0.00024411034246440977\n",
            "Average error of the model on the 190000 tactics positions is 0.00107302435208112\n",
            "Average error of the model on the 200000 tactics positions is 0.02891368791460991\n",
            "Average error of the model on the 210000 tactics positions is 0.00014359323540702462\n",
            "Average error of the model on the 220000 tactics positions is 0.018760759383440018\n",
            "Average error of the model on the 230000 tactics positions is 0.027271339669823647\n",
            "Average error of the model on the 240000 tactics positions is 0.00016851404507178813\n",
            "Average error of the model on the 250000 tactics positions is 0.01769772171974182\n",
            "Average error of the model on the 260000 tactics positions is 0.00017075186769943684\n",
            "Average error of the model on the 270000 tactics positions is 0.019394420087337494\n",
            "Average error of the model on the 280000 tactics positions is 0.007568071596324444\n",
            "Average error of the model on the 290000 tactics positions is 0.0001910540013341233\n",
            "Average error of the model on the 300000 tactics positions is 0.013418426737189293\n",
            "Average error of the model on the 300000 tactics positions is 0.013418426737189293\n",
            "time elapsed on AdamW: 0:10:47.947242\n",
            "--------------------------------------------------\n",
            "running SGD - 0.01\n",
            "Converting to pytorch Dataset...\n",
            "[1, 1] loss: 0.036\n",
            "[1, 1001] loss: 12.078\n",
            "[1, 2001] loss: 9.573\n",
            "[1, 3001] loss: 11.320\n",
            "[1, 4001] loss: 12.242\n",
            "[1, 5001] loss: 11.522\n",
            "[1, 6001] loss: nan\n",
            "[1, 7001] loss: nan\n",
            "[1, 8001] loss: nan\n",
            "[1, 9001] loss: nan\n",
            "[1, 10001] loss: nan\n",
            "[1, 11001] loss: nan\n",
            "[1, 12001] loss: nan\n",
            "[1, 13001] loss: nan\n",
            "[1, 14001] loss: nan\n",
            "[1, 15001] loss: nan\n",
            "[1, 16001] loss: nan\n",
            "[1, 17001] loss: nan\n",
            "[1, 18001] loss: nan\n",
            "[1, 19001] loss: nan\n",
            "[1, 20001] loss: nan\n",
            "[1, 21001] loss: nan\n",
            "[1, 22001] loss: nan\n",
            "[1, 23001] loss: nan\n",
            "[1, 24001] loss: nan\n",
            "[1, 25001] loss: nan\n",
            "[1, 26001] loss: nan\n",
            "[1, 27001] loss: nan\n",
            "[1, 28001] loss: nan\n",
            "[1, 29001] loss: nan\n",
            "[2, 1] loss: nan\n",
            "[2, 1001] loss: nan\n",
            "[2, 2001] loss: nan\n",
            "[2, 3001] loss: nan\n",
            "[2, 4001] loss: nan\n",
            "[2, 5001] loss: nan\n",
            "[2, 6001] loss: nan\n",
            "[2, 7001] loss: nan\n",
            "[2, 8001] loss: nan\n",
            "[2, 9001] loss: nan\n",
            "[2, 10001] loss: nan\n",
            "[2, 11001] loss: nan\n",
            "[2, 12001] loss: nan\n",
            "[2, 13001] loss: nan\n",
            "[2, 14001] loss: nan\n",
            "[2, 15001] loss: nan\n",
            "[2, 16001] loss: nan\n",
            "[2, 17001] loss: nan\n",
            "[2, 18001] loss: nan\n",
            "[2, 19001] loss: nan\n",
            "[2, 20001] loss: nan\n",
            "[2, 21001] loss: nan\n",
            "[2, 22001] loss: nan\n",
            "[2, 23001] loss: nan\n",
            "[2, 24001] loss: nan\n",
            "[2, 25001] loss: nan\n",
            "[2, 26001] loss: nan\n",
            "[2, 27001] loss: nan\n",
            "[2, 28001] loss: nan\n",
            "[2, 29001] loss: nan\n",
            "[3, 1] loss: nan\n",
            "[3, 1001] loss: nan\n",
            "[3, 2001] loss: nan\n",
            "[3, 3001] loss: nan\n",
            "[3, 4001] loss: nan\n",
            "[3, 5001] loss: nan\n",
            "[3, 6001] loss: nan\n",
            "[3, 7001] loss: nan\n",
            "[3, 8001] loss: nan\n",
            "[3, 9001] loss: nan\n",
            "[3, 10001] loss: nan\n",
            "[3, 11001] loss: nan\n",
            "[3, 12001] loss: nan\n",
            "[3, 13001] loss: nan\n",
            "[3, 14001] loss: nan\n",
            "[3, 15001] loss: nan\n",
            "[3, 16001] loss: nan\n",
            "[3, 17001] loss: nan\n",
            "[3, 18001] loss: nan\n",
            "[3, 19001] loss: nan\n",
            "[3, 20001] loss: nan\n",
            "[3, 21001] loss: nan\n",
            "[3, 22001] loss: nan\n",
            "[3, 23001] loss: nan\n",
            "[3, 24001] loss: nan\n",
            "[3, 25001] loss: nan\n",
            "[3, 26001] loss: nan\n",
            "[3, 27001] loss: nan\n",
            "[3, 28001] loss: nan\n",
            "[3, 29001] loss: nan\n",
            "[4, 1] loss: nan\n",
            "[4, 1001] loss: nan\n",
            "[4, 2001] loss: nan\n",
            "[4, 3001] loss: nan\n",
            "[4, 4001] loss: nan\n",
            "[4, 5001] loss: nan\n",
            "[4, 6001] loss: nan\n",
            "[4, 7001] loss: nan\n",
            "[4, 8001] loss: nan\n",
            "[4, 9001] loss: nan\n",
            "[4, 10001] loss: nan\n",
            "[4, 11001] loss: nan\n",
            "[4, 12001] loss: nan\n",
            "[4, 13001] loss: nan\n",
            "[4, 14001] loss: nan\n",
            "[4, 15001] loss: nan\n",
            "[4, 16001] loss: nan\n",
            "[4, 17001] loss: nan\n",
            "[4, 18001] loss: nan\n",
            "[4, 19001] loss: nan\n",
            "[4, 20001] loss: nan\n",
            "[4, 21001] loss: nan\n",
            "[4, 22001] loss: nan\n",
            "[4, 23001] loss: nan\n",
            "[4, 24001] loss: nan\n",
            "[4, 25001] loss: nan\n",
            "[4, 26001] loss: nan\n",
            "[4, 27001] loss: nan\n",
            "[4, 28001] loss: nan\n",
            "[4, 29001] loss: nan\n",
            "[5, 1] loss: nan\n",
            "[5, 1001] loss: nan\n",
            "[5, 2001] loss: nan\n",
            "[5, 3001] loss: nan\n",
            "[5, 4001] loss: nan\n",
            "[5, 5001] loss: nan\n",
            "[5, 6001] loss: nan\n",
            "[5, 7001] loss: nan\n",
            "[5, 8001] loss: nan\n",
            "[5, 9001] loss: nan\n",
            "[5, 10001] loss: nan\n",
            "[5, 11001] loss: nan\n",
            "[5, 12001] loss: nan\n",
            "[5, 13001] loss: nan\n",
            "[5, 14001] loss: nan\n",
            "[5, 15001] loss: nan\n",
            "[5, 16001] loss: nan\n",
            "[5, 17001] loss: nan\n",
            "[5, 18001] loss: nan\n",
            "[5, 19001] loss: nan\n",
            "[5, 20001] loss: nan\n",
            "[5, 21001] loss: nan\n",
            "[5, 22001] loss: nan\n",
            "[5, 23001] loss: nan\n",
            "[5, 24001] loss: nan\n",
            "[5, 25001] loss: nan\n",
            "[5, 26001] loss: nan\n",
            "[5, 27001] loss: nan\n",
            "[5, 28001] loss: nan\n",
            "[5, 29001] loss: nan\n",
            "[6, 1] loss: nan\n",
            "[6, 1001] loss: nan\n",
            "[6, 2001] loss: nan\n",
            "[6, 3001] loss: nan\n",
            "[6, 4001] loss: nan\n",
            "[6, 5001] loss: nan\n",
            "[6, 6001] loss: nan\n",
            "[6, 7001] loss: nan\n",
            "[6, 8001] loss: nan\n",
            "[6, 9001] loss: nan\n",
            "[6, 10001] loss: nan\n",
            "[6, 11001] loss: nan\n",
            "[6, 12001] loss: nan\n",
            "[6, 13001] loss: nan\n",
            "[6, 14001] loss: nan\n",
            "[6, 15001] loss: nan\n",
            "[6, 16001] loss: nan\n",
            "[6, 17001] loss: nan\n",
            "[6, 18001] loss: nan\n",
            "[6, 19001] loss: nan\n",
            "[6, 20001] loss: nan\n",
            "[6, 21001] loss: nan\n",
            "[6, 22001] loss: nan\n",
            "[6, 23001] loss: nan\n",
            "[6, 24001] loss: nan\n",
            "[6, 25001] loss: nan\n",
            "[6, 26001] loss: nan\n",
            "[6, 27001] loss: nan\n",
            "[6, 28001] loss: nan\n",
            "[6, 29001] loss: nan\n",
            "[7, 1] loss: nan\n",
            "[7, 1001] loss: nan\n",
            "[7, 2001] loss: nan\n",
            "[7, 3001] loss: nan\n",
            "[7, 4001] loss: nan\n",
            "[7, 5001] loss: nan\n",
            "[7, 6001] loss: nan\n",
            "[7, 7001] loss: nan\n",
            "[7, 8001] loss: nan\n",
            "[7, 9001] loss: nan\n",
            "[7, 10001] loss: nan\n",
            "[7, 11001] loss: nan\n",
            "[7, 12001] loss: nan\n",
            "[7, 13001] loss: nan\n",
            "[7, 14001] loss: nan\n",
            "[7, 15001] loss: nan\n",
            "[7, 16001] loss: nan\n",
            "[7, 17001] loss: nan\n",
            "[7, 18001] loss: nan\n",
            "[7, 19001] loss: nan\n",
            "[7, 20001] loss: nan\n",
            "[7, 21001] loss: nan\n",
            "[7, 22001] loss: nan\n",
            "[7, 23001] loss: nan\n",
            "[7, 24001] loss: nan\n",
            "[7, 25001] loss: nan\n",
            "[7, 26001] loss: nan\n",
            "[7, 27001] loss: nan\n",
            "[7, 28001] loss: nan\n",
            "[7, 29001] loss: nan\n",
            "[8, 1] loss: nan\n",
            "[8, 1001] loss: nan\n",
            "[8, 2001] loss: nan\n",
            "[8, 3001] loss: nan\n",
            "[8, 4001] loss: nan\n",
            "[8, 5001] loss: nan\n",
            "[8, 6001] loss: nan\n",
            "[8, 7001] loss: nan\n",
            "[8, 8001] loss: nan\n",
            "[8, 9001] loss: nan\n",
            "[8, 10001] loss: nan\n",
            "[8, 11001] loss: nan\n",
            "[8, 12001] loss: nan\n",
            "[8, 13001] loss: nan\n",
            "[8, 14001] loss: nan\n",
            "[8, 15001] loss: nan\n",
            "[8, 16001] loss: nan\n",
            "[8, 17001] loss: nan\n",
            "[8, 18001] loss: nan\n",
            "[8, 19001] loss: nan\n",
            "[8, 20001] loss: nan\n",
            "[8, 21001] loss: nan\n",
            "[8, 22001] loss: nan\n",
            "[8, 23001] loss: nan\n",
            "[8, 24001] loss: nan\n",
            "[8, 25001] loss: nan\n",
            "[8, 26001] loss: nan\n",
            "[8, 27001] loss: nan\n",
            "[8, 28001] loss: nan\n",
            "[8, 29001] loss: nan\n",
            "[9, 1] loss: nan\n",
            "[9, 1001] loss: nan\n",
            "[9, 2001] loss: nan\n",
            "[9, 3001] loss: nan\n",
            "[9, 4001] loss: nan\n",
            "[9, 5001] loss: nan\n",
            "[9, 6001] loss: nan\n",
            "[9, 7001] loss: nan\n",
            "[9, 8001] loss: nan\n",
            "[9, 9001] loss: nan\n",
            "[9, 10001] loss: nan\n",
            "[9, 11001] loss: nan\n",
            "[9, 12001] loss: nan\n",
            "[9, 13001] loss: nan\n",
            "[9, 14001] loss: nan\n",
            "[9, 15001] loss: nan\n",
            "[9, 16001] loss: nan\n",
            "[9, 17001] loss: nan\n",
            "[9, 18001] loss: nan\n",
            "[9, 19001] loss: nan\n",
            "[9, 20001] loss: nan\n",
            "[9, 21001] loss: nan\n",
            "[9, 22001] loss: nan\n",
            "[9, 23001] loss: nan\n",
            "[9, 24001] loss: nan\n",
            "[9, 25001] loss: nan\n",
            "[9, 26001] loss: nan\n",
            "[9, 27001] loss: nan\n",
            "[9, 28001] loss: nan\n",
            "[9, 29001] loss: nan\n",
            "[10, 1] loss: nan\n",
            "[10, 1001] loss: nan\n",
            "[10, 2001] loss: nan\n",
            "[10, 3001] loss: nan\n",
            "[10, 4001] loss: nan\n",
            "[10, 5001] loss: nan\n",
            "[10, 6001] loss: nan\n",
            "[10, 7001] loss: nan\n",
            "[10, 8001] loss: nan\n",
            "[10, 9001] loss: nan\n",
            "[10, 10001] loss: nan\n",
            "[10, 11001] loss: nan\n",
            "[10, 12001] loss: nan\n",
            "[10, 13001] loss: nan\n",
            "[10, 14001] loss: nan\n",
            "[10, 15001] loss: nan\n",
            "[10, 16001] loss: nan\n",
            "[10, 17001] loss: nan\n",
            "[10, 18001] loss: nan\n",
            "[10, 19001] loss: nan\n",
            "[10, 20001] loss: nan\n",
            "[10, 21001] loss: nan\n",
            "[10, 22001] loss: nan\n",
            "[10, 23001] loss: nan\n",
            "[10, 24001] loss: nan\n",
            "[10, 25001] loss: nan\n",
            "[10, 26001] loss: nan\n",
            "[10, 27001] loss: nan\n",
            "[10, 28001] loss: nan\n",
            "[10, 29001] loss: nan\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the model on the 10000 tactics positions is nan\n",
            "Average error of the model on the 20000 tactics positions is nan\n",
            "Average error of the model on the 30000 tactics positions is nan\n",
            "Average error of the model on the 40000 tactics positions is nan\n",
            "Average error of the model on the 50000 tactics positions is nan\n",
            "Average error of the model on the 60000 tactics positions is nan\n",
            "Average error of the model on the 70000 tactics positions is nan\n",
            "Average error of the model on the 80000 tactics positions is nan\n",
            "Average error of the model on the 90000 tactics positions is nan\n",
            "Average error of the model on the 100000 tactics positions is nan\n",
            "Average error of the model on the 110000 tactics positions is nan\n",
            "Average error of the model on the 120000 tactics positions is nan\n",
            "Average error of the model on the 130000 tactics positions is nan\n",
            "Average error of the model on the 140000 tactics positions is nan\n",
            "Average error of the model on the 150000 tactics positions is nan\n",
            "Average error of the model on the 160000 tactics positions is nan\n",
            "Average error of the model on the 170000 tactics positions is nan\n",
            "Average error of the model on the 180000 tactics positions is nan\n",
            "Average error of the model on the 190000 tactics positions is nan\n",
            "Average error of the model on the 200000 tactics positions is nan\n",
            "Average error of the model on the 210000 tactics positions is nan\n",
            "Average error of the model on the 220000 tactics positions is nan\n",
            "Average error of the model on the 230000 tactics positions is nan\n",
            "Average error of the model on the 240000 tactics positions is nan\n",
            "Average error of the model on the 250000 tactics positions is nan\n",
            "Average error of the model on the 260000 tactics positions is nan\n",
            "Average error of the model on the 270000 tactics positions is nan\n",
            "Average error of the model on the 280000 tactics positions is nan\n",
            "Average error of the model on the 290000 tactics positions is nan\n",
            "Average error of the model on the 300000 tactics positions is nan\n",
            "Average error of the model on the 300000 tactics positions is nan\n",
            "time elapsed on SGD - 0.01: 0:07:51.428100\n",
            "--------------------------------------------------\n",
            "running SGD - 0.001\n",
            "Converting to pytorch Dataset...\n",
            "[1, 1] loss: 0.000\n",
            "[1, 1001] loss: 10.744\n",
            "[1, 2001] loss: 11.382\n",
            "[1, 3001] loss: 10.498\n",
            "[1, 4001] loss: 12.954\n",
            "[1, 5001] loss: 11.881\n",
            "[1, 6001] loss: 13.513\n",
            "[1, 7001] loss: 12.552\n",
            "[1, 8001] loss: 11.171\n",
            "[1, 9001] loss: 11.863\n",
            "[1, 10001] loss: 11.030\n",
            "[1, 11001] loss: 12.573\n",
            "[1, 12001] loss: 10.145\n",
            "[1, 13001] loss: 12.574\n",
            "[1, 14001] loss: 10.939\n",
            "[1, 15001] loss: 11.537\n",
            "[1, 16001] loss: 12.033\n",
            "[1, 17001] loss: 10.597\n",
            "[1, 18001] loss: 11.346\n",
            "[1, 19001] loss: 11.583\n",
            "[1, 20001] loss: 10.817\n",
            "[1, 21001] loss: 11.203\n",
            "[1, 22001] loss: 10.083\n",
            "[1, 23001] loss: 9.459\n",
            "[1, 24001] loss: 10.831\n",
            "[1, 25001] loss: 11.101\n",
            "[1, 26001] loss: 9.934\n",
            "[1, 27001] loss: 10.685\n",
            "[1, 28001] loss: 9.702\n",
            "[1, 29001] loss: 10.815\n",
            "[2, 1] loss: 0.003\n",
            "[2, 1001] loss: 11.925\n",
            "[2, 2001] loss: 11.196\n",
            "[2, 3001] loss: 10.028\n",
            "[2, 4001] loss: 9.970\n",
            "[2, 5001] loss: 9.852\n",
            "[2, 6001] loss: 11.421\n",
            "[2, 7001] loss: 10.397\n",
            "[2, 8001] loss: 9.738\n",
            "[2, 9001] loss: 9.515\n",
            "[2, 10001] loss: 9.825\n",
            "[2, 11001] loss: 8.517\n",
            "[2, 12001] loss: 7.836\n",
            "[2, 13001] loss: 10.621\n",
            "[2, 14001] loss: 10.962\n",
            "[2, 15001] loss: 11.995\n",
            "[2, 16001] loss: 11.097\n",
            "[2, 17001] loss: 12.270\n",
            "[2, 18001] loss: 10.560\n",
            "[2, 19001] loss: 11.467\n",
            "[2, 20001] loss: 11.838\n",
            "[2, 21001] loss: 11.228\n",
            "[2, 22001] loss: 12.016\n",
            "[2, 23001] loss: 13.117\n",
            "[2, 24001] loss: 12.331\n",
            "[2, 25001] loss: 10.688\n",
            "[2, 26001] loss: 11.654\n",
            "[2, 27001] loss: 12.416\n",
            "[2, 28001] loss: 10.763\n",
            "[2, 29001] loss: 10.747\n",
            "[3, 1] loss: 0.001\n",
            "[3, 1001] loss: 10.250\n",
            "[3, 2001] loss: 10.843\n",
            "[3, 3001] loss: 11.216\n",
            "[3, 4001] loss: 11.684\n",
            "[3, 5001] loss: 11.425\n",
            "[3, 6001] loss: 11.952\n",
            "[3, 7001] loss: 10.875\n",
            "[3, 8001] loss: 12.501\n",
            "[3, 9001] loss: 11.152\n",
            "[3, 10001] loss: 9.976\n",
            "[3, 11001] loss: 10.498\n",
            "[3, 12001] loss: 11.602\n",
            "[3, 13001] loss: 10.966\n",
            "[3, 14001] loss: 12.109\n",
            "[3, 15001] loss: 11.417\n",
            "[3, 16001] loss: 10.053\n",
            "[3, 17001] loss: 12.818\n",
            "[3, 18001] loss: 12.014\n",
            "[3, 19001] loss: 11.578\n",
            "[3, 20001] loss: 10.010\n",
            "[3, 21001] loss: 10.815\n",
            "[3, 22001] loss: 13.398\n",
            "[3, 23001] loss: 12.792\n",
            "[3, 24001] loss: 10.948\n",
            "[3, 25001] loss: 12.107\n",
            "[3, 26001] loss: 12.489\n",
            "[3, 27001] loss: 11.441\n",
            "[3, 28001] loss: 12.772\n",
            "[3, 29001] loss: 12.147\n",
            "[4, 1] loss: 0.020\n",
            "[4, 1001] loss: 11.395\n",
            "[4, 2001] loss: 12.634\n",
            "[4, 3001] loss: 12.300\n",
            "[4, 4001] loss: 11.246\n",
            "[4, 5001] loss: 11.887\n",
            "[4, 6001] loss: 13.096\n",
            "[4, 7001] loss: 11.732\n",
            "[4, 8001] loss: 11.488\n",
            "[4, 9001] loss: 10.628\n",
            "[4, 10001] loss: 11.266\n",
            "[4, 11001] loss: 12.393\n",
            "[4, 12001] loss: 12.226\n",
            "[4, 13001] loss: 12.720\n",
            "[4, 14001] loss: 10.524\n",
            "[4, 15001] loss: 12.502\n",
            "[4, 16001] loss: 9.754\n",
            "[4, 17001] loss: 12.576\n",
            "[4, 18001] loss: 10.249\n",
            "[4, 19001] loss: 10.127\n",
            "[4, 20001] loss: 10.355\n",
            "[4, 21001] loss: 13.161\n",
            "[4, 22001] loss: 11.097\n",
            "[4, 23001] loss: 10.629\n",
            "[4, 24001] loss: 11.496\n",
            "[4, 25001] loss: 12.009\n",
            "[4, 26001] loss: 11.631\n",
            "[4, 27001] loss: 12.277\n",
            "[4, 28001] loss: 11.105\n",
            "[4, 29001] loss: 10.272\n",
            "[5, 1] loss: 0.000\n",
            "[5, 1001] loss: 10.270\n",
            "[5, 2001] loss: 11.957\n",
            "[5, 3001] loss: 10.154\n",
            "[5, 4001] loss: 11.150\n",
            "[5, 5001] loss: 13.097\n",
            "[5, 6001] loss: 11.953\n",
            "[5, 7001] loss: 11.989\n",
            "[5, 8001] loss: 11.311\n",
            "[5, 9001] loss: 12.162\n",
            "[5, 10001] loss: 12.103\n",
            "[5, 11001] loss: 10.857\n",
            "[5, 12001] loss: 12.875\n",
            "[5, 13001] loss: 12.690\n",
            "[5, 14001] loss: 11.882\n",
            "[5, 15001] loss: 9.450\n",
            "[5, 16001] loss: 11.671\n",
            "[5, 17001] loss: 11.240\n",
            "[5, 18001] loss: 12.518\n",
            "[5, 19001] loss: 11.884\n",
            "[5, 20001] loss: 9.873\n",
            "[5, 21001] loss: 11.657\n",
            "[5, 22001] loss: 12.651\n",
            "[5, 23001] loss: 11.933\n",
            "[5, 24001] loss: 11.759\n",
            "[5, 25001] loss: 10.511\n",
            "[5, 26001] loss: 11.787\n",
            "[5, 27001] loss: 11.635\n",
            "[5, 28001] loss: 10.760\n",
            "[5, 29001] loss: 11.386\n",
            "[6, 1] loss: 0.000\n",
            "[6, 1001] loss: 11.954\n",
            "[6, 2001] loss: 10.155\n",
            "[6, 3001] loss: 10.510\n",
            "[6, 4001] loss: 11.408\n",
            "[6, 5001] loss: 11.890\n",
            "[6, 6001] loss: 11.805\n",
            "[6, 7001] loss: 10.765\n",
            "[6, 8001] loss: 10.293\n",
            "[6, 9001] loss: 11.642\n",
            "[6, 10001] loss: 12.293\n",
            "[6, 11001] loss: 12.741\n",
            "[6, 12001] loss: 12.098\n",
            "[6, 13001] loss: 11.763\n",
            "[6, 14001] loss: 11.256\n",
            "[6, 15001] loss: 11.664\n",
            "[6, 16001] loss: 11.349\n",
            "[6, 17001] loss: 10.493\n",
            "[6, 18001] loss: 11.892\n",
            "[6, 19001] loss: 11.229\n",
            "[6, 20001] loss: 10.712\n",
            "[6, 21001] loss: 12.334\n",
            "[6, 22001] loss: 12.483\n",
            "[6, 23001] loss: 10.590\n",
            "[6, 24001] loss: 12.595\n",
            "[6, 25001] loss: 12.025\n",
            "[6, 26001] loss: 10.736\n",
            "[6, 27001] loss: 11.611\n",
            "[6, 28001] loss: 10.185\n",
            "[6, 29001] loss: 12.453\n",
            "[7, 1] loss: 0.005\n",
            "[7, 1001] loss: 12.422\n",
            "[7, 2001] loss: 12.384\n",
            "[7, 3001] loss: 13.288\n",
            "[7, 4001] loss: 12.095\n",
            "[7, 5001] loss: 11.211\n",
            "[7, 6001] loss: 11.822\n",
            "[7, 7001] loss: 12.718\n",
            "[7, 8001] loss: 12.031\n",
            "[7, 9001] loss: 13.448\n",
            "[7, 10001] loss: 10.825\n",
            "[7, 11001] loss: 12.590\n",
            "[7, 12001] loss: 10.590\n",
            "[7, 13001] loss: 10.312\n",
            "[7, 14001] loss: 11.198\n",
            "[7, 15001] loss: 9.478\n",
            "[7, 16001] loss: 9.824\n",
            "[7, 17001] loss: 10.812\n",
            "[7, 18001] loss: 12.320\n",
            "[7, 19001] loss: 10.696\n",
            "[7, 20001] loss: 12.498\n",
            "[7, 21001] loss: 11.296\n",
            "[7, 22001] loss: 10.456\n",
            "[7, 23001] loss: 10.995\n",
            "[7, 24001] loss: 10.971\n",
            "[7, 25001] loss: 12.538\n",
            "[7, 26001] loss: 11.249\n",
            "[7, 27001] loss: 10.500\n",
            "[7, 28001] loss: 11.323\n",
            "[7, 29001] loss: 11.242\n",
            "[8, 1] loss: 0.000\n",
            "[8, 1001] loss: 10.818\n",
            "[8, 2001] loss: 10.340\n",
            "[8, 3001] loss: 10.336\n",
            "[8, 4001] loss: 11.816\n",
            "[8, 5001] loss: 12.381\n",
            "[8, 6001] loss: 11.101\n",
            "[8, 7001] loss: 11.570\n",
            "[8, 8001] loss: 10.707\n",
            "[8, 9001] loss: 11.882\n",
            "[8, 10001] loss: 12.384\n",
            "[8, 11001] loss: 11.343\n",
            "[8, 12001] loss: 11.741\n",
            "[8, 13001] loss: 10.758\n",
            "[8, 14001] loss: 12.025\n",
            "[8, 15001] loss: 10.030\n",
            "[8, 16001] loss: 11.412\n",
            "[8, 17001] loss: 11.263\n",
            "[8, 18001] loss: 11.840\n",
            "[8, 19001] loss: 11.651\n",
            "[8, 20001] loss: 11.463\n",
            "[8, 21001] loss: 11.006\n",
            "[8, 22001] loss: 12.541\n",
            "[8, 23001] loss: 12.253\n",
            "[8, 24001] loss: 13.500\n",
            "[8, 25001] loss: 10.669\n",
            "[8, 26001] loss: 12.781\n",
            "[8, 27001] loss: 10.377\n",
            "[8, 28001] loss: 11.189\n",
            "[8, 29001] loss: 13.132\n",
            "[9, 1] loss: 0.003\n",
            "[9, 1001] loss: 11.983\n",
            "[9, 2001] loss: 11.337\n",
            "[9, 3001] loss: 12.342\n",
            "[9, 4001] loss: 12.032\n",
            "[9, 5001] loss: 13.126\n",
            "[9, 6001] loss: 10.971\n",
            "[9, 7001] loss: 12.136\n",
            "[9, 8001] loss: 10.288\n",
            "[9, 9001] loss: 10.906\n",
            "[9, 10001] loss: 12.124\n",
            "[9, 11001] loss: 10.603\n",
            "[9, 12001] loss: 11.398\n",
            "[9, 13001] loss: 9.818\n",
            "[9, 14001] loss: 11.100\n",
            "[9, 15001] loss: 13.117\n",
            "[9, 16001] loss: 10.390\n",
            "[9, 17001] loss: 10.161\n",
            "[9, 18001] loss: 10.422\n",
            "[9, 19001] loss: 14.345\n",
            "[9, 20001] loss: 10.972\n",
            "[9, 21001] loss: 11.786\n",
            "[9, 22001] loss: 10.947\n",
            "[9, 23001] loss: 9.420\n",
            "[9, 24001] loss: 10.732\n",
            "[9, 25001] loss: 11.398\n",
            "[9, 26001] loss: 12.492\n",
            "[9, 27001] loss: 14.125\n",
            "[9, 28001] loss: 11.814\n",
            "[9, 29001] loss: 10.380\n",
            "[10, 1] loss: 0.000\n",
            "[10, 1001] loss: 11.330\n",
            "[10, 2001] loss: 12.449\n",
            "[10, 3001] loss: 11.382\n",
            "[10, 4001] loss: 11.740\n",
            "[10, 5001] loss: 10.654\n",
            "[10, 6001] loss: 12.250\n",
            "[10, 7001] loss: 11.614\n",
            "[10, 8001] loss: 11.789\n",
            "[10, 9001] loss: 11.094\n",
            "[10, 10001] loss: 10.459\n",
            "[10, 11001] loss: 10.928\n",
            "[10, 12001] loss: 11.123\n",
            "[10, 13001] loss: 12.832\n",
            "[10, 14001] loss: 10.965\n",
            "[10, 15001] loss: 12.966\n",
            "[10, 16001] loss: 9.674\n",
            "[10, 17001] loss: 12.302\n",
            "[10, 18001] loss: 12.431\n",
            "[10, 19001] loss: 10.193\n",
            "[10, 20001] loss: 10.034\n",
            "[10, 21001] loss: 10.983\n",
            "[10, 22001] loss: 11.855\n",
            "[10, 23001] loss: 11.279\n",
            "[10, 24001] loss: 11.412\n",
            "[10, 25001] loss: 11.993\n",
            "[10, 26001] loss: 11.446\n",
            "[10, 27001] loss: 13.550\n",
            "[10, 28001] loss: 13.130\n",
            "[10, 29001] loss: 10.848\n",
            "Finished Training\n",
            "Evaluating model\n",
            "Average error of the model on the 10000 tactics positions is 0.10365915298461914\n",
            "Average error of the model on the 20000 tactics positions is 0.0013103388482704759\n",
            "Average error of the model on the 30000 tactics positions is 0.0006598493782803416\n",
            "Average error of the model on the 40000 tactics positions is 0.00043008546344935894\n",
            "Average error of the model on the 50000 tactics positions is 0.0006381772691383958\n",
            "Average error of the model on the 60000 tactics positions is 0.0008056394290179014\n",
            "Average error of the model on the 70000 tactics positions is 0.05824447423219681\n",
            "Average error of the model on the 80000 tactics positions is 0.025807159021496773\n",
            "Average error of the model on the 90000 tactics positions is 0.02314283698797226\n",
            "Average error of the model on the 100000 tactics positions is 0.020641237497329712\n",
            "Average error of the model on the 110000 tactics positions is 0.036871448159217834\n",
            "Average error of the model on the 120000 tactics positions is 0.0002355189499212429\n",
            "Average error of the model on the 130000 tactics positions is 0.0545184500515461\n",
            "Average error of the model on the 140000 tactics positions is 0.0434129573404789\n",
            "Average error of the model on the 150000 tactics positions is 0.020436156541109085\n",
            "Average error of the model on the 160000 tactics positions is 0.01899855211377144\n",
            "Average error of the model on the 170000 tactics positions is 0.017526041716337204\n",
            "Average error of the model on the 180000 tactics positions is 0.0002373989118495956\n",
            "Average error of the model on the 190000 tactics positions is 0.00044601099216379225\n",
            "Average error of the model on the 200000 tactics positions is 0.03017670288681984\n",
            "Average error of the model on the 210000 tactics positions is 0.00010465680679772049\n",
            "Average error of the model on the 220000 tactics positions is 0.018038837239146233\n",
            "Average error of the model on the 230000 tactics positions is 0.026469696313142776\n",
            "Average error of the model on the 240000 tactics positions is 0.00013762251182924956\n",
            "Average error of the model on the 250000 tactics positions is 0.016320595517754555\n",
            "Average error of the model on the 260000 tactics positions is 0.00013902400678489357\n",
            "Average error of the model on the 270000 tactics positions is 0.018782202154397964\n",
            "Average error of the model on the 280000 tactics positions is 0.0071541061624884605\n",
            "Average error of the model on the 290000 tactics positions is 0.00014747613749932498\n",
            "Average error of the model on the 300000 tactics positions is 0.013539263047277927\n",
            "Average error of the model on the 300000 tactics positions is 0.013539263047277927\n",
            "time elapsed on SGD - 0.001: 0:07:49.617760\n",
            "--------------------------------------------------\n",
            "AdamW:  tensor(0.0134, device='cuda:0')\n",
            "SGD - 0.01:  tensor(nan, device='cuda:0')\n",
            "SGD - 0.001:  tensor(0.0135, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "models_300k = run_models(300_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(nan, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(models_300k[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ft_BO1EkEN-"
      },
      "source": [
        "# Resultados de AdamW\n",
        "\n",
        "Um erro médio de 0,0225 parece muito razoável, porém isto não é tão surpreendente quanto poderia parecer a princípio, já que as posições onde a avaliação do stockfish não são um par forçado, foram normalizadas para uma faixa de aproximadamente -1,5 a 1,5 e as posições onde a avaliação do stockfish é um par forçado foram definidas para -100 ou 100. Por convenção, uma pontuação positiva indica uma vantagem para o branco e uma pontuação negativa indica uma vantagem para o preto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqaMOztFkEN_"
      },
      "source": [
        "# Testando com SGD\n",
        "\n",
        "Originalmente usamos o algoritmo AdamW para a etapa de otimização, mas e se tentarmos a Descendência Estocástica Gradiente (SGD)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToqT2gEAkEOA"
      },
      "source": [
        "Um erro médio de 0,0207 é uma ligeira melhoria, mas não tanto que possamos dizer definitivamente que o SGD fez melhor do que AdamW. Como a perda de treinamento não diminuiu muito ao longo das 10 épocas, é muito mais provável que o SGD seja um otimizador pior para este problema de classificação ou que a taxa de aprendizado seja muito grande, o que está impedindo o modelo de convergir para o ótimo local. Vamos tentar novamente, mas com uma taxa de aprendizado de 0,001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhpZCgdzkEOA"
      },
      "source": [
        "O erro de treinamento parece estar diminuindo mesmo que esporádico, mas o erro de classificação ficou um pouco pior. Parece que o AdamW é simplesmente mais consistente e converge mais rapidamente para um ótimo local."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNpeA0kXkEOA"
      },
      "source": [
        "# Conclusão\n",
        "\n",
        "AdamW parece ser um algoritmo mais consistente e que converge mais rapidamente para um ótimo local baseado na comparação de AdamW, SGD(lr=0,01), e SGD(lr=0,001)\n",
        "\n",
        "Isto significa que o melhor modelo teve um erro médio de 0,0225, em oposição ao erro médio mais baixo de 0,0207, que não é suficientemente significativo para justificar a inconsistência do otimizador SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3EYal1VkEOB"
      },
      "source": [
        "# Desafios\n",
        "\n",
        "Este conjunto de dados (ou pelo menos o conjunto de dados chessData.csv) tem mais de 16 milhões de posições e avaliações, o que torna bastante difícil trabalhar com ele. Por causa disso, tive que limitar a quantidade de dados realmente utilizada a 200.000 posições, 100.000 no conjunto de dados de treinamento e 100.000 no conjunto de dados de teste. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFtEg1g4kEOB"
      },
      "source": [
        "# Referências\n",
        "\n",
        "1. https://www.kaggle.com/ronakbadhe/chess-evaluation-prediction\n",
        "2. https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation\n",
        "3. http://starship-knowledge.com/wp-content/uploads/2020/10/Perceptrons-1024x724.jpeg\n",
        "4. https://starship-knowledge.com/wp-content/uploads/2020/10/Perceptrons-1024x724.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GidrTuPLkEOB"
      },
      "source": [
        "# Contribution\n",
        "\n",
        "* Representation of castling rights, en passant, active color, halfmoves and fullmoves in an 8x8 grid\n",
        "* Neural Network Architecture\n",
        "* Testing AdamW vs SVG"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chess-position-evaluator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "8e3105560c81ae39e0834f5867d71e8b951b6beb57a3746bf5585c30fcba755d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
